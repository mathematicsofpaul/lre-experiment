{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dea318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b592d5",
   "metadata": {},
   "source": [
    "## Setup: Load Model and Tokenizer\n",
    "\n",
    "We'll use GPT-2 for this demo since it's universally accessible and doesn't require authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43514bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading gpt2...\n",
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  # Using GPT-2 for universal compatibility\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39331410",
   "metadata": {},
   "source": [
    "## Create a Few-Shot Prompt Template\n",
    "\n",
    "We'll create a prompt with:\n",
    "- 3 few-shot examples\n",
    "- The test subject \"wine\" at the end\n",
    "\n",
    "Note: \"wine\" appears multiple times in the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc8706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Prompt:\n",
      "================================================================================\n",
      "water is commonly associated with liquid.\n",
      "gold is commonly associated with metal.\n",
      "wine is commonly associated with red.\n",
      "wine is commonly associated with\n",
      "================================================================================\n",
      "\n",
      "Test subject: 'wine'\n",
      "Note: 'wine' appears TWICE in this prompt!\n",
      "  - Once in the examples (line 3)\n",
      "  - Once in the test prompt (line 4)\n",
      "\n",
      "We want to extract from the LAST occurrence (line 4)!\n"
     ]
    }
   ],
   "source": [
    "# Few-shot examples\n",
    "examples = [\n",
    "    (\"water\", \"liquid\"),\n",
    "    (\"gold\", \"metal\"),\n",
    "    (\"wine\", \"red\"),  # Note: \"wine\" appears here in examples!\n",
    "]\n",
    "\n",
    "# Template\n",
    "template = \"{} is commonly associated with\"\n",
    "\n",
    "# Test subject\n",
    "test_subject = \"wine\"\n",
    "\n",
    "# Build the full few-shot prompt\n",
    "few_shot_part = \"\\n\".join([\n",
    "    template.format(subj) + f\" {obj}.\"\n",
    "    for subj, obj in examples\n",
    "])\n",
    "\n",
    "test_prompt_part = template.format(test_subject)\n",
    "\n",
    "full_prompt = few_shot_part + \"\\n\" + test_prompt_part\n",
    "\n",
    "print(\"Full Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(full_prompt)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest subject: '{test_subject}'\")\n",
    "print(f\"Note: '{test_subject}' appears TWICE in this prompt!\")\n",
    "print(f\"  - Once in the examples (line 3)\")\n",
    "print(f\"  - Once in the test prompt (line 4)\")\n",
    "print(f\"\\nWe want to extract from the LAST occurrence (line 4)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af29b25",
   "metadata": {},
   "source": [
    "## Step 1: Tokenize the Prompt\n",
    "\n",
    "Standard tokenization shows us the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8385597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [12987, 374, 16626, 5815, 448, 14473, 624, 34537, 374, 16626, 5815, 448, 9317, 624, 71437, 374, 16626, 5815, 448, 2518, 624, 71437, 374, 16626, 5815, 448]\n",
      "Total tokens: 26\n",
      "\n",
      "Token-by-token breakdown:\n",
      "--------------------------------------------------------------------------------\n",
      "Token  0: ID  12987 → 'water'\n",
      "Token  1: ID    374 → ' is'\n",
      "Token  2: ID  16626 → ' commonly'\n",
      "Token  3: ID   5815 → ' associated'\n",
      "Token  4: ID    448 → ' with'\n",
      "Token  5: ID  14473 → ' liquid'\n",
      "Token  6: ID    624 → '.\n",
      "'\n",
      "Token  7: ID  34537 → 'gold'\n",
      "Token  8: ID    374 → ' is'\n",
      "Token  9: ID  16626 → ' commonly'\n",
      "Token 10: ID   5815 → ' associated'\n",
      "Token 11: ID    448 → ' with'\n",
      "Token 12: ID   9317 → ' metal'\n",
      "Token 13: ID    624 → '.\n",
      "'\n",
      "Token 14: ID  71437 → 'wine'\n",
      "Token 15: ID    374 → ' is'\n",
      "Token 16: ID  16626 → ' commonly'\n",
      "Token 17: ID   5815 → ' associated'\n",
      "Token 18: ID    448 → ' with'\n",
      "Token 19: ID   2518 → ' red'\n",
      "Token 20: ID    624 → '.\n",
      "'\n",
      "Token 21: ID  71437 → 'wine'\n",
      "Token 22: ID    374 → ' is'\n",
      "Token 23: ID  16626 → ' commonly'\n",
      "Token 24: ID   5815 → ' associated'\n",
      "Token 25: ID    448 → ' with'\n"
     ]
    }
   ],
   "source": [
    "# Standard tokenization\n",
    "prompt_tokens = tokenizer.encode(full_prompt, add_special_tokens=False)\n",
    "\n",
    "print(f\"Token IDs: {prompt_tokens}\")\n",
    "print(f\"Total tokens: {len(prompt_tokens)}\")\n",
    "print(f\"\\nToken-by-token breakdown:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, token_id in enumerate(prompt_tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"Token {i:2d}: ID {token_id:6d} → '{token_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8a1be",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize with Offset Mapping\n",
    "\n",
    "**This is the key!** Offset mapping tells us the character positions for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31495d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset Mapping (character positions for each token):\n",
      "================================================================================\n",
      "Format: Token [start_char, end_char) → 'text'\n",
      "--------------------------------------------------------------------------------\n",
      "Token  0: [  0,   5) → 'water'  (chars: 'water')\n",
      "Token  1: [  5,   8) → ' is'  (chars: ' is')\n",
      "Token  2: [  8,  17) → ' commonly'  (chars: ' commonly')\n",
      "Token  3: [ 17,  28) → ' associated'  (chars: ' associated')\n",
      "Token  4: [ 28,  33) → ' with'  (chars: ' with')\n",
      "Token  5: [ 33,  40) → ' liquid'  (chars: ' liquid')\n",
      "Token  6: [ 40,  42) → '.\n",
      "'  (chars: '.\n",
      "')\n",
      "Token  7: [ 42,  46) → 'gold'  (chars: 'gold')\n",
      "Token  8: [ 46,  49) → ' is'  (chars: ' is')\n",
      "Token  9: [ 49,  58) → ' commonly'  (chars: ' commonly')\n",
      "Token 10: [ 58,  69) → ' associated'  (chars: ' associated')\n",
      "Token 11: [ 69,  74) → ' with'  (chars: ' with')\n",
      "Token 12: [ 74,  80) → ' metal'  (chars: ' metal')\n",
      "Token 13: [ 80,  82) → '.\n",
      "'  (chars: '.\n",
      "')\n",
      "Token 14: [ 82,  86) → 'wine'  (chars: 'wine')\n",
      "Token 15: [ 86,  89) → ' is'  (chars: ' is')\n",
      "Token 16: [ 89,  98) → ' commonly'  (chars: ' commonly')\n",
      "Token 17: [ 98, 109) → ' associated'  (chars: ' associated')\n",
      "Token 18: [109, 114) → ' with'  (chars: ' with')\n",
      "Token 19: [114, 118) → ' red'  (chars: ' red')\n",
      "Token 20: [118, 120) → '.\n",
      "'  (chars: '.\n",
      "')\n",
      "Token 21: [120, 124) → 'wine'  (chars: 'wine')\n",
      "Token 22: [124, 127) → ' is'  (chars: ' is')\n",
      "Token 23: [127, 136) → ' commonly'  (chars: ' commonly')\n",
      "Token 24: [136, 147) → ' associated'  (chars: ' associated')\n",
      "Token 25: [147, 152) → ' with'  (chars: ' with')\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with offset mapping\n",
    "inputs_with_offsets = tokenizer(\n",
    "    full_prompt, \n",
    "    return_tensors=\"pt\", \n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "offset_mapping = inputs_with_offsets[\"offset_mapping\"][0]  # Shape: (seq_len, 2)\n",
    "\n",
    "print(\"Offset Mapping (character positions for each token):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Format: Token [start_char, end_char) → 'text'\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, (start, end) in enumerate(offset_mapping):\n",
    "    token_text = tokenizer.decode([prompt_tokens[i]])\n",
    "    chars = full_prompt[start:end] if start < len(full_prompt) else \"\"\n",
    "    print(f\"Token {i:2d}: [{start:3d}, {end:3d}) → '{token_text}'  (chars: '{chars}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24036877",
   "metadata": {},
   "source": [
    "## Step 3: Find Subject in the String\n",
    "\n",
    "Use `rfind()` to find the **LAST** occurrence of the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86720999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for subject: 'wine'\n",
      "================================================================================\n",
      "\n",
      "All occurrences of 'wine' in the prompt:\n",
      "  Occurrence 1: chars [82, 86) → ...ociated with metal.\n",
      "wine is commonly associa...\n",
      "  Occurrence 2: chars [120, 124) → ...ssociated with red.\n",
      "wine is commonly associa...\n",
      "\n",
      "→ Using rfind() to get LAST occurrence:\n",
      "  Character range: [120, 124)\n",
      "  Text: 'wine'\n",
      "\n",
      "Context around the subject:\n",
      "  'commonly associated with red.\n",
      "wine is commonly associated with'\n",
      "                                ↑↑↑↑\n",
      "                                Subject here\n"
     ]
    }
   ],
   "source": [
    "# Find the subject in the prompt string\n",
    "print(f\"Looking for subject: '{test_subject}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find ALL occurrences\n",
    "print(\"\\nAll occurrences of 'wine' in the prompt:\")\n",
    "start_pos = 0\n",
    "occurrence_num = 1\n",
    "while True:\n",
    "    pos = full_prompt.find(test_subject, start_pos)\n",
    "    if pos == -1:\n",
    "        break\n",
    "    end_pos = pos + len(test_subject)\n",
    "    # Show context\n",
    "    context_start = max(0, pos - 20)\n",
    "    context_end = min(len(full_prompt), end_pos + 20)\n",
    "    context = full_prompt[context_start:context_end]\n",
    "    print(f\"  Occurrence {occurrence_num}: chars [{pos}, {end_pos}) → ...{context}...\")\n",
    "    start_pos = pos + 1\n",
    "    occurrence_num += 1\n",
    "\n",
    "# Find the LAST occurrence\n",
    "subject_start_char = full_prompt.rfind(test_subject)\n",
    "subject_end_char = subject_start_char + len(test_subject)\n",
    "\n",
    "print(f\"\\n→ Using rfind() to get LAST occurrence:\")\n",
    "print(f\"  Character range: [{subject_start_char}, {subject_end_char})\")\n",
    "print(f\"  Text: '{full_prompt[subject_start_char:subject_end_char]}'\")\n",
    "\n",
    "# Show context\n",
    "context_start = max(0, subject_start_char - 30)\n",
    "context_end = min(len(full_prompt), subject_end_char + 30)\n",
    "context = full_prompt[context_start:context_end]\n",
    "marker_pos = subject_start_char - context_start\n",
    "\n",
    "print(f\"\\nContext around the subject:\")\n",
    "print(f\"  '{context}'\")\n",
    "print(f\"  {' ' * marker_pos}{'↑' * len(test_subject)}\")\n",
    "print(f\"  {' ' * marker_pos}Subject here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94febca",
   "metadata": {},
   "source": [
    "## Step 4: Map Character Positions to Token Indices\n",
    "\n",
    "Find which tokens overlap with the subject's character range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9def232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tokens that overlap with subject:\n",
      "================================================================================\n",
      "Subject character range: [120, 124)\n",
      "\n",
      "Checking each token:\n",
      "--------------------------------------------------------------------------------\n",
      "  Token 12: [ 74,  80) no overlap → ' metal'\n",
      "  Token 13: [ 80,  82) no overlap → '.\n",
      "'\n",
      "  Token 14: [ 82,  86) no overlap → 'wine'\n",
      "  Token 15: [ 86,  89) no overlap → ' is'\n",
      "  Token 16: [ 89,  98) no overlap → ' commonly'\n",
      "  Token 17: [ 98, 109) no overlap → ' associated'\n",
      "  Token 18: [109, 114) no overlap → ' with'\n",
      "  Token 19: [114, 118) no overlap → ' red'\n",
      "  Token 20: [118, 120) no overlap → '.\n",
      "'\n",
      "✓ Token 21: [120, 124) overlaps! → 'wine'\n",
      "  Token 22: [124, 127) no overlap → ' is'\n",
      "  Token 23: [127, 136) no overlap → ' commonly'\n",
      "  Token 24: [136, 147) no overlap → ' associated'\n",
      "  Token 25: [147, 152) no overlap → ' with'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Overlapping tokens: [21]\n",
      "→ Last overlapping token: 21\n",
      "→ Token text: 'wine'\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding tokens that overlap with subject:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Subject character range: [{subject_start_char}, {subject_end_char})\")\n",
    "print(\"\\nChecking each token:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "subject_last_token = None\n",
    "overlapping_tokens = []\n",
    "\n",
    "for token_idx, (start, end) in enumerate(offset_mapping):\n",
    "    # Check if this token overlaps with the subject's character range\n",
    "    overlaps = start < subject_end_char and end > subject_start_char\n",
    "    \n",
    "    token_text = tokenizer.decode([prompt_tokens[token_idx]])\n",
    "    \n",
    "    if overlaps:\n",
    "        subject_last_token = token_idx  # Keep updating - we want the LAST one\n",
    "        overlapping_tokens.append(token_idx)\n",
    "        print(f\"✓ Token {token_idx:2d}: [{start:3d}, {end:3d}) overlaps! → '{token_text}'\")\n",
    "    else:\n",
    "        # Only show tokens near the subject\n",
    "        if abs(start - subject_start_char) < 50:\n",
    "            print(f\"  Token {token_idx:2d}: [{start:3d}, {end:3d}) no overlap → '{token_text}'\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nOverlapping tokens: {overlapping_tokens}\")\n",
    "print(f\"→ Last overlapping token: {subject_last_token}\")\n",
    "print(f\"→ Token text: '{tokenizer.decode([prompt_tokens[subject_last_token]])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0d73b",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Extraction Point\n",
    "\n",
    "Show tokens around the extraction point with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5b0717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context around the extraction point:\n",
      "================================================================================\n",
      "\n",
      "Tokens 16 to 25:\n",
      "--------------------------------------------------------------------------------\n",
      "     Token 16: [ 89,  98) → ' commonly'\n",
      "     Token 17: [ 98, 109) → ' associated'\n",
      "     Token 18: [109, 114) → ' with'\n",
      "     Token 19: [114, 118) → ' red'\n",
      "     Token 20: [118, 120) → '.\n",
      "'\n",
      ">>>> Token 21: [120, 124) → 'wine' *** EXTRACT HERE ***\n",
      "     Token 22: [124, 127) → ' is'\n",
      "     Token 23: [127, 136) → ' commonly'\n",
      "     Token 24: [136, 147) → ' associated'\n",
      "     Token 25: [147, 152) → ' with'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ We extract the hidden state from Token 21\n",
      "  This is the LAST token of the subject 'wine' in the test prompt!\n"
     ]
    }
   ],
   "source": [
    "print(\"Context around the extraction point:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show tokens around the extraction point\n",
    "start_idx = max(0, subject_last_token - 5)\n",
    "end_idx = min(len(prompt_tokens), subject_last_token + 6)\n",
    "\n",
    "print(f\"\\nTokens {start_idx} to {end_idx-1}:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i in range(start_idx, end_idx):\n",
    "    token_text = tokenizer.decode([prompt_tokens[i]])\n",
    "    char_range = offset_mapping[i]\n",
    "    \n",
    "    if i == subject_last_token:\n",
    "        marker = \">>>>\"\n",
    "        highlight = \" *** EXTRACT HERE ***\"\n",
    "    elif i in overlapping_tokens:\n",
    "        marker = \"  →\"\n",
    "        highlight = \" (part of subject)\"\n",
    "    else:\n",
    "        marker = \"    \"\n",
    "        highlight = \"\"\n",
    "    \n",
    "    print(f\"{marker} Token {i:2d}: [{char_range[0]:3d}, {char_range[1]:3d}) → '{token_text}'{highlight}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"\\n✓ We extract the hidden state from Token {subject_last_token}\")\n",
    "print(f\"  This is the LAST token of the subject 'wine' in the test prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190747b3",
   "metadata": {},
   "source": [
    "## Step 6: Verify with LREModel.find_subject_last_token()\n",
    "\n",
    "Confirm that the `find_subject_last_token()` method gives the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a10ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ") on Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-0.6B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")...\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlre\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LREModel\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize LRE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m lre = \u001b[43mLREModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the method\u001b[39;00m\n\u001b[32m      7\u001b[39m lre_position = lre.find_subject_last_token(full_prompt, test_subject)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/lre/lre.py:14\u001b[39m, in \u001b[36mLREModel.__init__\u001b[39m\u001b[34m(self, model_name, device, token)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     16\u001b[39m     model_name, \n\u001b[32m     17\u001b[39m     token=token,\n\u001b[32m     18\u001b[39m     local_files_only=\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Allow downloads and show progress\u001b[39;00m\n\u001b[32m     19\u001b[39m ).to(device)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1089\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1091\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    918\u001b[39m     token = use_auth_token\n\u001b[32m    920\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    938\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have -- or .. in repo_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'."
     ]
    }
   ],
   "source": [
    "from lre import LREModel\n",
    "\n",
    "# Initialize LRE\n",
    "lre = LREModel(model, tokenizer, device)\n",
    "\n",
    "# Use the method\n",
    "lre_position = lre.find_subject_last_token(full_prompt, test_subject)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Manual calculation:        Token {subject_last_token}\")\n",
    "print(f\"LREModel method result:    Token {lre_position}\")\n",
    "print()\n",
    "\n",
    "if lre_position == subject_last_token:\n",
    "    print(\"✓ ✓ ✓ PERFECT MATCH! ✓ ✓ ✓\")\n",
    "    print(\"\\nThe find_subject_last_token() method works correctly!\")\n",
    "else:\n",
    "    print(f\"✗ MISMATCH! Difference: {abs(lre_position - subject_last_token)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0697c1",
   "metadata": {},
   "source": [
    "## Summary: How It All Works Together\n",
    "\n",
    "### The Problem:\n",
    "- Subject \"wine\" appears MULTIPLE times in few-shot prompts\n",
    "- We need to extract from the LAST occurrence (the test prompt, not examples)\n",
    "- Separately tokenizing the subject can give different tokens due to context\n",
    "\n",
    "### The Solution (Offset Mapping):\n",
    "1. **Tokenize with offsets**: Get character positions for each token\n",
    "2. **Find subject in string**: Use `rfind()` to get LAST occurrence\n",
    "3. **Map chars to tokens**: Find which tokens overlap with subject\n",
    "4. **Return last token**: The last overlapping token is what we want!\n",
    "\n",
    "### Why It's Robust:\n",
    "- ✓ Uses the ACTUAL tokenized prompt (no tokenization mismatch)\n",
    "- ✓ Character positions are ground truth (unambiguous)\n",
    "- ✓ Finds LAST occurrence automatically (critical for few-shot)\n",
    "- ✓ Handles multi-token subjects correctly (maps all overlapping tokens)\n",
    "- ✓ Context-aware (subject is already in its final tokenized form)\n",
    "\n",
    "### Used by:\n",
    "- `train_lre()`: Extracts subject hidden states for training\n",
    "- `evaluate()`: Extracts subject hidden states for prediction\n",
    "- Both use `extract_from=\"subject_end\"` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6272ff08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_loo_prompt' from 'data_utils' (/Users/paulnguyen/lre-experiment/data_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_loo_prompt\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# LOO (Leave-One-Out) Template\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# In LOO, we REMOVE the test example from the few-shot examples\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOriginal Few-Shot Examples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'create_loo_prompt' from 'data_utils' (/Users/paulnguyen/lre-experiment/data_utils.py)"
     ]
    }
   ],
   "source": [
    "from data_utils import create_loo_prompt\n",
    "\n",
    "# LOO (Leave-One-Out) Template\n",
    "# In LOO, we REMOVE the test example from the few-shot examples\n",
    "\n",
    "print(\"Original Few-Shot Examples:\")\n",
    "print(\"=\"*80)\n",
    "for subj, obj in examples:\n",
    "    print(f\"  {subj} → {obj}\")\n",
    "\n",
    "print(f\"\\nTest subject: {test_subject}\")\n",
    "print(f\"Test subject matches example: {test_subject in [subj for subj, obj in examples]}\")\n",
    "\n",
    "# Create LOO prompt using data_utils\n",
    "loo_full_prompt = create_loo_prompt(examples, test_subject, template)\n",
    "\n",
    "# Extract LOO examples for display\n",
    "loo_examples = [(subj, obj) for subj, obj in examples if subj != test_subject]\n",
    "\n",
    "print(f\"\\nLOO Examples (removed '{test_subject}'):\")\n",
    "print(\"=\"*80)\n",
    "for subj, obj in loo_examples:\n",
    "    print(f\"  {subj} → {obj}\")\n",
    "\n",
    "print(\"\\nLOO Full Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(loo_full_prompt)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nKey Difference:\")\n",
    "print(f\"  Original prompt: '{test_subject}' appears {full_prompt.count(test_subject)} times\")\n",
    "print(f\"  LOO prompt:      '{test_subject}' appears {loo_full_prompt.count(test_subject)} time(s)\")\n",
    "print(f\"\\n✓ LOO ensures the model only sees '{test_subject}' in the test position!\")\n",
    "print(f\"  This prevents the model from simply copying from the examples.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lre-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
