{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f169f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "sys.path.append('/Users/paulnguyen/lre-experiment')\n",
    "from lre import LREModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a26786",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b74220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model loaded: Qwen/Qwen3-0.6B\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d0f8d",
   "metadata": {},
   "source": [
    "## Test Case 1: Simple Template (Subject at Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df220752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'wine is commonly associated with'\n",
      "\n",
      "Subject: 'wine'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple template where subject is at the beginning\n",
    "template_simple = \"{} is commonly associated with\"\n",
    "subject = \"wine\"\n",
    "prompt = template_simple.format(subject)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nSubject: '{subject}'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cf772ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full prompt tokens: [71437, 374, 16626, 5815, 448]\n",
      "Number of tokens in full prompt: 5\n",
      "\n",
      "Token-by-token breakdown:\n",
      "  Position 0: Token ID 71437 → 'wine'\n",
      "  Position 1: Token ID   374 → ' is'\n",
      "  Position 2: Token ID 16626 → ' commonly'\n",
      "  Position 3: Token ID  5815 → ' associated'\n",
      "  Position 4: Token ID   448 → ' with'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the full prompt\n",
    "prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "print(f\"\\nFull prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Number of tokens in full prompt: {len(prompt_tokens)}\")\n",
    "\n",
    "# Show what each token decodes to\n",
    "print(\"\\nToken-by-token breakdown:\")\n",
    "for i, token_id in enumerate(prompt_tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  Position {i}: Token ID {token_id:5d} → '{token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37cbe91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject 'wine' tokens: [71437]\n",
      "Number of tokens in subject: 1\n",
      "\n",
      "Subject token breakdown:\n",
      "  Token 0: ID 71437 → 'wine'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the subject separately\n",
    "subject_tokens = tokenizer.encode(subject, add_special_tokens=False)\n",
    "print(f\"\\nSubject '{subject}' tokens: {subject_tokens}\")\n",
    "print(f\"Number of tokens in subject: {len(subject_tokens)}\")\n",
    "\n",
    "# Show subject token breakdown\n",
    "print(\"\\nSubject token breakdown:\")\n",
    "for i, token_id in enumerate(subject_tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  Token {i}: ID {token_id:5d} → '{token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5234343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OFFSET MAPPING APPROACH (Used in lre.py)\n",
      "================================================================================\n",
      "\n",
      "Prompt: 'wine is commonly associated with'\n",
      "Subject: 'wine'\n",
      "\n",
      "Offset mapping (character positions for each token):\n",
      "  Token 0: chars [ 0,  4) → 'wine'\n",
      "  Token 1: chars [ 4,  7) → ' is'\n",
      "  Token 2: chars [ 7, 16) → ' commonly'\n",
      "  Token 3: chars [16, 27) → ' associated'\n",
      "  Token 4: chars [27, 32) → ' with'\n",
      "\n",
      "Subject 'wine' character positions:\n",
      "  Start: 0\n",
      "  End: 4\n",
      "  Characters [0, 4): 'wine'\n",
      "\n",
      "Finding tokens that overlap with subject:\n",
      "  Token 0: chars [ 0,  4) overlaps! → 'wine' → LAST TOKEN OF SUBJECT\n",
      "\n",
      "✓ Subject's last token is at position: 0\n",
      "  Token ID: 71437\n",
      "  Token text: 'wine'\n",
      "\n",
      "================================================================================\n",
      "WHY THIS WORKS:\n",
      "================================================================================\n",
      "✓ Uses character positions from the ACTUAL tokenized prompt\n",
      "✓ No tokenization mismatch (subject is already in context)\n",
      "✓ Finds LAST occurrence (important for few-shot templates)\n",
      "✓ Handles multi-token subjects correctly\n"
     ]
    }
   ],
   "source": [
    "# CORRECT APPROACH: Use offset mapping to find subject's last token\n",
    "# This is what find_subject_last_token() does in lre.py\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OFFSET MAPPING APPROACH (Used in lre.py)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Tokenize with offset mapping to get character positions\n",
    "inputs_with_offsets = tokenizer(prompt, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "offset_mapping = inputs_with_offsets[\"offset_mapping\"][0]  # Shape: (seq_len, 2)\n",
    "\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "print(f\"Subject: '{subject}'\")\n",
    "print(f\"\\nOffset mapping (character positions for each token):\")\n",
    "for i, (start, end) in enumerate(offset_mapping):\n",
    "    token_text = tokenizer.decode([prompt_tokens[i]])\n",
    "    print(f\"  Token {i}: chars [{start:2d}, {end:2d}) → '{token_text}'\")\n",
    "\n",
    "# Step 2: Find subject's character position in the prompt (last occurrence)\n",
    "subject_start_char = prompt.rfind(subject)\n",
    "subject_end_char = subject_start_char + len(subject)\n",
    "\n",
    "print(f\"\\nSubject '{subject}' character positions:\")\n",
    "print(f\"  Start: {subject_start_char}\")\n",
    "print(f\"  End: {subject_end_char}\")\n",
    "print(f\"  Characters [{subject_start_char}, {subject_end_char}): '{prompt[subject_start_char:subject_end_char]}'\")\n",
    "\n",
    "# Step 3: Find which tokens overlap with the subject\n",
    "subject_last_token = None\n",
    "print(f\"\\nFinding tokens that overlap with subject:\")\n",
    "for token_idx, (start, end) in enumerate(offset_mapping):\n",
    "    # Check if this token overlaps with the subject's character range\n",
    "    overlaps = start < subject_end_char and end > subject_start_char\n",
    "    if overlaps:\n",
    "        subject_last_token = token_idx  # Keep updating - we want the LAST one\n",
    "        marker = \"→ LAST TOKEN OF SUBJECT\" if token_idx == len(offset_mapping) - 1 or not any(\n",
    "            s < subject_end_char and e > subject_start_char \n",
    "            for s, e in offset_mapping[token_idx+1:]\n",
    "        ) else \"\"\n",
    "        token_text = tokenizer.decode([prompt_tokens[token_idx]])\n",
    "        print(f\"  Token {token_idx}: chars [{start:2d}, {end:2d}) overlaps! → '{token_text}' {marker}\")\n",
    "\n",
    "if subject_last_token is None:\n",
    "    raise ValueError(f\"Could not find subject '{subject}' in tokenized prompt\")\n",
    "\n",
    "print(f\"\\n✓ Subject's last token is at position: {subject_last_token}\")\n",
    "print(f\"  Token ID: {prompt_tokens[subject_last_token]}\")\n",
    "print(f\"  Token text: '{tokenizer.decode([prompt_tokens[subject_last_token]])}'\")\n",
    "\n",
    "# Store for later use\n",
    "subject_last_token_pos = subject_last_token\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"WHY THIS WORKS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Uses character positions from the ACTUAL tokenized prompt\")\n",
    "print(\"✓ No tokenization mismatch (subject is already in context)\")\n",
    "print(\"✓ Finds LAST occurrence (important for few-shot templates)\")\n",
    "print(\"✓ Handles multi-token subjects correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d8468",
   "metadata": {},
   "source": [
    "## Test Case 2: Few-Shot Template (Subject in Middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cdb8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "oil is commonly associated with fuel.\n",
      "juice is commonly associated with orange.\n",
      "wine is commonly associated with\n",
      "\n",
      "Subject: 'wine'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Few-shot template with examples before the subject\n",
    "template_fewshot = \"oil is commonly associated with fuel.\\njuice is commonly associated with orange.\\n{} is commonly associated with\"\n",
    "subject2 = \"wine\"\n",
    "prompt2 = template_fewshot.format(subject2)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt2}\")\n",
    "print(f\"\\nSubject: '{subject2}'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56f0d192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full prompt tokens: [73813, 374, 16626, 5815, 448, 10416, 624, 8613, 558, 374, 16626, 5815, 448, 18575, 624, 71437, 374, 16626, 5815, 448]\n",
      "Number of tokens in full prompt: 20\n",
      "\n",
      "Token-by-token breakdown:\n",
      "  Position  0: Token ID 73813 → 'oil'\n",
      "  Position  1: Token ID   374 → ' is'\n",
      "  Position  2: Token ID 16626 → ' commonly'\n",
      "  Position  3: Token ID  5815 → ' associated'\n",
      "  Position  4: Token ID   448 → ' with'\n",
      "  Position  5: Token ID 10416 → ' fuel'\n",
      "  Position  6: Token ID   624 → '.\n",
      "'\n",
      "  Position  7: Token ID  8613 → 'ju'\n",
      "  Position  8: Token ID   558 → 'ice'\n",
      "  Position  9: Token ID   374 → ' is'\n",
      "  Position 10: Token ID 16626 → ' commonly'\n",
      "  Position 11: Token ID  5815 → ' associated'\n",
      "  Position 12: Token ID   448 → ' with'\n",
      "  Position 13: Token ID 18575 → ' orange'\n",
      "  Position 14: Token ID   624 → '.\n",
      "'\n",
      "  Position 15: Token ID 71437 → 'wine'\n",
      "  Position 16: Token ID   374 → ' is'\n",
      "  Position 17: Token ID 16626 → ' commonly'\n",
      "  Position 18: Token ID  5815 → ' associated'\n",
      "  Position 19: Token ID   448 → ' with'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the full prompt\n",
    "prompt2_tokens = tokenizer.encode(prompt2, add_special_tokens=False)\n",
    "print(f\"\\nFull prompt tokens: {prompt2_tokens}\")\n",
    "print(f\"Number of tokens in full prompt: {len(prompt2_tokens)}\")\n",
    "\n",
    "# Show what each token decodes to\n",
    "print(\"\\nToken-by-token breakdown:\")\n",
    "for i, token_id in enumerate(prompt2_tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  Position {i:2d}: Token ID {token_id:5d} → '{token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbee5bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject 'wine' tokens: [71437]\n",
      "Number of tokens in subject: 1\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the subject separately\n",
    "subject2_tokens = tokenizer.encode(subject2, add_special_tokens=False)\n",
    "print(f\"\\nSubject '{subject2}' tokens: {subject2_tokens}\")\n",
    "print(f\"Number of tokens in subject: {len(subject2_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f80134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OFFSET MAPPING WITH FEW-SHOT TEMPLATE\n",
      "================================================================================\n",
      "\n",
      "Prompt (first 100 chars): oil is commonly associated with fuel.\n",
      "juice is commonly associated with orange.\n",
      "wine is commonly ass...\n",
      "Subject: 'wine'\n",
      "\n",
      "Subject 'wine' character positions:\n",
      "  Start: 80\n",
      "  End: 84\n",
      "  Text: 'wine'\n",
      "\n",
      "Context: 'monly associated with orange.\n",
      "wine is commonly associated with'\n",
      "                                        ↑↑↑↑ (subject here)\n",
      "\n",
      "Finding tokens that overlap with subject:\n",
      "  Token 15: chars [ 80,  84) → 'wine'\n",
      "\n",
      "→ Subject's last token is at position: 15\n",
      "→ Token ID: 71437\n",
      "→ Token text: 'wine'\n",
      "\n",
      "Context around extraction point:\n",
      "     Token 13: chars [ 71,  78) → ' orange' \n",
      "     Token 14: chars [ 78,  80) → '.\n",
      "' \n",
      "  → Token 15: chars [ 80,  84) → 'wine' *** EXTRACT HERE ***\n",
      "     Token 16: chars [ 84,  87) → ' is' \n",
      "     Token 17: chars [ 87,  96) → ' commonly' \n",
      "\n",
      "================================================================================\n",
      "CRITICAL: rfind() finds the LAST occurrence!\n",
      "================================================================================\n",
      "In few-shot templates, 'wine' appears multiple times:\n",
      "  - In examples: 'wine is...red.'\n",
      "  - In test prompt: 'wine is commonly associated with'\n",
      "We want the LAST one (in the test prompt), not the examples!\n",
      "rfind() ensures we get the right occurrence.\n"
     ]
    }
   ],
   "source": [
    "# Use offset mapping for few-shot template (shows why this is critical!)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OFFSET MAPPING WITH FEW-SHOT TEMPLATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Tokenize with offset mapping\n",
    "inputs_with_offsets2 = tokenizer(prompt2, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "offset_mapping2 = inputs_with_offsets2[\"offset_mapping\"][0]\n",
    "\n",
    "print(f\"\\nPrompt (first 100 chars): {prompt2[:100]}...\")\n",
    "print(f\"Subject: '{subject2}'\")\n",
    "\n",
    "# Step 2: Find subject's character position (LAST occurrence!)\n",
    "subject2_start_char = prompt2.rfind(subject2)\n",
    "subject2_end_char = subject2_start_char + len(subject2)\n",
    "\n",
    "print(f\"\\nSubject '{subject2}' character positions:\")\n",
    "print(f\"  Start: {subject2_start_char}\")\n",
    "print(f\"  End: {subject2_end_char}\")\n",
    "print(f\"  Text: '{prompt2[subject2_start_char:subject2_end_char]}'\")\n",
    "\n",
    "# Show context around subject in the string\n",
    "context_start = max(0, subject2_start_char - 30)\n",
    "context_end = min(len(prompt2), subject2_end_char + 30)\n",
    "context = prompt2[context_start:context_end]\n",
    "subject_pos_in_context = subject2_start_char - context_start\n",
    "print(f\"\\nContext: '{context}'\")\n",
    "print(f\"          {' ' * subject_pos_in_context}{'↑' * len(subject2)} (subject here)\")\n",
    "\n",
    "# Step 3: Find overlapping tokens\n",
    "subject2_last_token = None\n",
    "print(f\"\\nFinding tokens that overlap with subject:\")\n",
    "for token_idx, (start, end) in enumerate(offset_mapping2):\n",
    "    overlaps = start < subject2_end_char and end > subject2_start_char\n",
    "    if overlaps:\n",
    "        subject2_last_token = token_idx\n",
    "        token_text = tokenizer.decode([prompt2_tokens[token_idx]])\n",
    "        print(f\"  Token {token_idx}: chars [{start:3d}, {end:3d}) → '{token_text}'\")\n",
    "\n",
    "if subject2_last_token is None:\n",
    "    raise ValueError(f\"Could not find subject '{subject2}' in tokenized prompt\")\n",
    "\n",
    "print(f\"\\n→ Subject's last token is at position: {subject2_last_token}\")\n",
    "print(f\"→ Token ID: {prompt2_tokens[subject2_last_token]}\")\n",
    "print(f\"→ Token text: '{tokenizer.decode([prompt2_tokens[subject2_last_token]])}'\")\n",
    "\n",
    "# Show context around this token\n",
    "print(f\"\\nContext around extraction point:\")\n",
    "start = max(0, subject2_last_token - 2)\n",
    "end = min(len(prompt2_tokens), subject2_last_token + 3)\n",
    "for j in range(start, end):\n",
    "    marker = \"→\" if j == subject2_last_token else \"  \"\n",
    "    highlight = \"*** EXTRACT HERE ***\" if j == subject2_last_token else \"\"\n",
    "    token_text = tokenizer.decode([prompt2_tokens[j]])\n",
    "    char_range = offset_mapping2[j]\n",
    "    print(f\"  {marker} Token {j:2d}: chars [{char_range[0]:3d}, {char_range[1]:3d}) → '{token_text}' {highlight}\")\n",
    "\n",
    "subject2_last_token_pos = subject2_last_token\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CRITICAL: rfind() finds the LAST occurrence!\")\n",
    "print(\"=\"*80)\n",
    "print(\"In few-shot templates, 'wine' appears multiple times:\")\n",
    "print(\"  - In examples: 'wine is...red.'\")\n",
    "print(\"  - In test prompt: 'wine is commonly associated with'\")\n",
    "print(\"We want the LAST one (in the test prompt), not the examples!\")\n",
    "print(\"rfind() ensures we get the right occurrence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa9f2d",
   "metadata": {},
   "source": [
    "## Test Case 3: Using the LRE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91168c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ") on Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-0.6B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")...\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test that lre.find_subject_last_token() gives the same result\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m lre = \u001b[43mLREModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Test with simple template\u001b[39;00m\n\u001b[32m      6\u001b[39m layer_name = \u001b[33m\"\u001b[39m\u001b[33mmodel.layers.10\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/lre/lre.py:14\u001b[39m, in \u001b[36mLREModel.__init__\u001b[39m\u001b[34m(self, model_name, device, token)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     16\u001b[39m     model_name, \n\u001b[32m     17\u001b[39m     token=token,\n\u001b[32m     18\u001b[39m     local_files_only=\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Allow downloads and show progress\u001b[39;00m\n\u001b[32m     19\u001b[39m ).to(device)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1089\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1091\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:921\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    918\u001b[39m     token = use_auth_token\n\u001b[32m    920\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    938\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m The name cannot start or end with \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and the maximum length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have -- or .. in repo_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: 'Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)'."
     ]
    }
   ],
   "source": [
    "# Test that lre.find_subject_last_token() gives the same result\n",
    "\n",
    "lre = LREModel(model, tokenizer, device)\n",
    "\n",
    "# Test with simple template\n",
    "layer_name = \"model.layers.10\"\n",
    "print(f\"Testing lre.find_subject_last_token() method\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "# Get the position using the LRE method\n",
    "lre_position = lre.find_subject_last_token(prompt, subject)\n",
    "\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "print(f\"Subject: '{subject}'\")\n",
    "print(f\"\\nManual calculation: position {subject_last_token_pos}\")\n",
    "print(f\"LRE method result:  position {lre_position}\")\n",
    "\n",
    "if lre_position == subject_last_token_pos:\n",
    "    print(f\"\\n✓ MATCH! Both methods found the same position.\")\n",
    "else:\n",
    "    print(f\"\\n✗ MISMATCH! Different positions found.\")\n",
    "    print(f\"  Difference: {abs(lre_position - subject_last_token_pos)} tokens\")\n",
    "\n",
    "# Test with few-shot template too\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing with few-shot template:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "lre_position2 = lre.find_subject_last_token(prompt2, subject2)\n",
    "\n",
    "print(f\"\\nPrompt (first 100 chars): {prompt2[:100]}...\")\n",
    "print(f\"Subject: '{subject2}'\")\n",
    "print(f\"\\nManual calculation: position {subject2_last_token_pos}\")\n",
    "print(f\"LRE method result:  position {lre_position2}\")\n",
    "\n",
    "if lre_position2 == subject2_last_token_pos:\n",
    "    print(f\"\\n✓ MATCH! Both methods found the same position.\")\n",
    "else:\n",
    "    print(f\"\\n✗ MISMATCH! Different positions found.\")\n",
    "    print(f\"  Difference: {abs(lre_position2 - subject2_last_token_pos)} tokens\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"This is exactly how train_lre() and evaluate() work internally!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4227f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING HIDDEN STATE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEXTRACTING HIDDEN STATE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m hidden_state = \u001b[43mlre\u001b[49m.get_hidden_state(prompt, layer_name, token_position=subject_last_token_pos)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracted from layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToken position: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_last_token_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'lre' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract hidden state using the correct position\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING HIDDEN STATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hidden_state = lre.get_hidden_state(prompt, layer_name, token_position=subject_last_token_pos)\n",
    "\n",
    "print(f\"\\nExtracted from layer: {layer_name}\")\n",
    "print(f\"Token position: {subject_last_token_pos}\")\n",
    "print(f\"Token: '{tokenizer.decode([prompt_tokens[subject_last_token_pos]])}'\")\n",
    "print(f\"\\n✓ Hidden state shape: {hidden_state.shape}\")\n",
    "print(f\"  Hidden state norm: {np.linalg.norm(hidden_state):.4f}\")\n",
    "print(f\"  First 5 values: {hidden_state[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ae0d7",
   "metadata": {},
   "source": [
    "## Test Case 4: Compare with Template End Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17f750b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract from template end (last token of prompt)\u001b[39;00m\n\u001b[32m      2\u001b[39m last_token_pos = \u001b[38;5;28mlen\u001b[39m(prompt_tokens) - \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m hidden_state_template_end = \u001b[43mlre\u001b[49m.get_hidden_state(prompt, layer_name, token_position=\u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtraction from TEMPLATE END (position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_token_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Token: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer.decode([prompt_tokens[last_token_pos]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'lre' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract from template end (last token of prompt)\n",
    "last_token_pos = len(prompt_tokens) - 1\n",
    "hidden_state_template_end = lre.get_hidden_state(prompt, layer_name, token_position=\"last\")\n",
    "\n",
    "print(f\"Extraction from TEMPLATE END (position {last_token_pos}):\")\n",
    "print(f\"  Token: '{tokenizer.decode([prompt_tokens[last_token_pos]])}'\")\n",
    "print(f\"  Hidden state norm: {torch.norm(hidden_state_template_end).item():.4f}\")\n",
    "print(f\"\\nExtraction from SUBJECT END (position {subject_last_token_pos}):\")\n",
    "print(f\"  Token: '{tokenizer.decode([prompt_tokens[subject_last_token_pos]])}'\")\n",
    "print(f\"  Hidden state norm: {torch.norm(hidden_state).item():.4f}\")\n",
    "\n",
    "# Check if they're different\n",
    "difference = torch.norm(hidden_state - hidden_state_template_end).item()\n",
    "print(f\"\\nDifference between the two: {difference:.4f}\")\n",
    "if difference > 0.001:\n",
    "    print(\"✓ The two extraction points give DIFFERENT hidden states (as expected!)\")\n",
    "else:\n",
    "    print(\"⚠ The two extraction points give the SAME hidden state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478d5f1",
   "metadata": {},
   "source": [
    "## Test Case 5: Multi-Token Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddb9c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'New York is commonly associated with'\n",
      "Subject: 'New York'\n",
      "================================================================================\n",
      "\n",
      "Full prompt tokens: [3564, 4261, 374, 16626, 5815, 448]\n",
      "Subject tokens: [3564, 4261]\n",
      "Subject has 2 tokens\n",
      "\n",
      "✓ Subject spans positions 0 to 1\n",
      "→ Extracting from position 1 (last token of subject)\n",
      "→ Token text: ' York'\n"
     ]
    }
   ],
   "source": [
    "# Test with a multi-token subject\n",
    "subject_multi = \"New York\"\n",
    "prompt_multi = template_simple.format(subject_multi)\n",
    "\n",
    "print(f\"Prompt: '{prompt_multi}'\")\n",
    "print(f\"Subject: '{subject_multi}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenize\n",
    "prompt_multi_tokens = tokenizer.encode(prompt_multi, add_special_tokens=False)\n",
    "subject_multi_tokens = tokenizer.encode(subject_multi, add_special_tokens=False)\n",
    "\n",
    "print(f\"\\nFull prompt tokens: {prompt_multi_tokens}\")\n",
    "print(f\"Subject tokens: {subject_multi_tokens}\")\n",
    "print(f\"Subject has {len(subject_multi_tokens)} tokens\")\n",
    "\n",
    "# Find subject\n",
    "subject_multi_len = len(subject_multi_tokens)\n",
    "for i in range(len(prompt_multi_tokens) - subject_multi_len + 1):\n",
    "    if prompt_multi_tokens[i:i+subject_multi_len] == subject_multi_tokens:\n",
    "        subject_multi_start = i\n",
    "        subject_multi_last = i + subject_multi_len - 1\n",
    "        print(f\"\\n✓ Subject spans positions {subject_multi_start} to {subject_multi_last}\")\n",
    "        print(f\"→ Extracting from position {subject_multi_last} (last token of subject)\")\n",
    "        print(f\"→ Token text: '{tokenizer.decode([prompt_multi_tokens[subject_multi_last]])}'\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06883e97",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE WORKFLOW SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. OFFSET MAPPING APPROACH (find_subject_last_token):\")\n",
    "print(\"   ✓ Tokenize prompt with return_offsets_mapping=True\")\n",
    "print(\"   ✓ Get character positions: [(0,4), (4,7), ...]\")\n",
    "print(\"   ✓ Find subject in string: prompt.rfind(subject)\")\n",
    "print(\"   ✓ Map character positions to token indices\")\n",
    "print(\"   ✓ Return last token that overlaps with subject\")\n",
    "\n",
    "print(\"\\n2. WHY IT WORKS:\")\n",
    "print(\"   ✓ No tokenization mismatch (uses actual tokenized prompt)\")\n",
    "print(\"   ✓ Context-aware (subject already in its final tokenized form)\")\n",
    "print(\"   ✓ Finds LAST occurrence (critical for few-shot templates)\")\n",
    "print(\"   ✓ Handles multi-token subjects correctly\")\n",
    "\n",
    "print(\"\\n3. HOW train_lre() USES IT:\")\n",
    "print(\"   For each training sample:\")\n",
    "print(\"     a. Format prompt: template.format(subject)\")\n",
    "print(\"     b. Find position: find_subject_last_token(prompt, subject)\")\n",
    "print(\"     c. Extract hidden state: get_hidden_state(prompt, layer, position)\")\n",
    "print(\"     d. Get target: embedding(object)\")\n",
    "print(\"     e. Fit: W * h_subject + b ≈ embedding(object)\")\n",
    "\n",
    "print(\"\\n4. HOW evaluate() USES IT:\")\n",
    "print(\"   For each test sample:\")\n",
    "print(\"     a. Format prompt: template.format(subject)\")\n",
    "print(\"     b. Find position: find_subject_last_token(prompt, subject)\")\n",
    "print(\"     c. Extract hidden state: get_hidden_state(prompt, layer, position)\")\n",
    "print(\"     d. Predict: z_pred = W * h_subject + b\")\n",
    "print(\"     e. Decode: argmax(z_pred @ embedding_matrix.T)\")\n",
    "print(\"     f. Compare with expected object\")\n",
    "\n",
    "print(\"\\n5. KEY ADVANTAGE OVER TEMPLATE SPLITTING:\")\n",
    "print(\"   OLD: Split template at {}, count tokens before/after\")\n",
    "print(\"     → Can fail if tokenizer treats parts differently\")\n",
    "print(\"   NEW: Use character offsets from actual tokenization\")\n",
    "print(\"     → Always correct because it uses the real tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"This notebook demonstrates the EXACT logic used in lre/lre.py!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lre-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
