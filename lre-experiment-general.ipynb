{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Experiment with Gemma-3-270m\n",
    "\n",
    "This notebook experiments with the smaller Gemma-3-270m model (270m parameters, 18 layers) and tests how different layers affect LRE faithfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from lre import LREModel\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Gemma3-270m has 18 layers (0-17)\n",
    "\n",
    "We'll test layers at different depths to understand where relational knowledge is encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47 relation files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3dcbfa31a048bc8c2c7ef58e60739a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Data File:', options=('bias/characteristic_gender.json', 'bias/degree_gender.json', 'bia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7af50f10d8476db535f3f55a4f240c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('google/gemma-3-1b-pt', 'google/gemma-3-270m', 'gpt2', 'gpt2-xl', 'met…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Get all JSON files from all subdirectories under data/\n",
    "data_root = \"data\"\n",
    "json_files = []\n",
    "\n",
    "if os.path.exists(data_root):\n",
    "    for subdir, dirs, files in os.walk(data_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                # Store relative path from data root\n",
    "                rel_path = os.path.relpath(os.path.join(subdir, file), data_root)\n",
    "                json_files.append(rel_path)\n",
    "\n",
    "json_files.sort()  # Sort for consistent ordering\n",
    "\n",
    "print(f\"Found {len(json_files)} relation files:\")\n",
    "\n",
    "# Create dropdown for selecting a data file\n",
    "data_file_dropdown = widgets.Dropdown(\n",
    "    options=json_files,\n",
    "    value=json_files[0] if json_files else None,\n",
    "    description='Data File:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create dropdown for selecting a model\n",
    "model_options = [\n",
    "    \"google/gemma-3-1b-pt\",\n",
    "    \"google/gemma-3-270m\",\n",
    "    \"gpt2\",\n",
    "    \"gpt2-xl\",\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Qwen/Qwen-1_8B\"\n",
    "]\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=\"google/gemma-3-1b-pt\",\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_file_dropdown)\n",
    "display(model_dropdown)\n",
    "\n",
    "# Create a variable that updates with selection\n",
    "def get_selected_file():\n",
    "    return os.path.join(data_root, data_file_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: google/gemma-3-1b-pt\n",
      "\n",
      "Selected data file: data/bias/characteristic_gender.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = model_dropdown.value\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "TEMPLATE = \"{} is commonly associated with\"\n",
    "DATA_FILE = os.path.join(data_root, data_file_dropdown.value)\n",
    "print(f\"\\nSelected data file: {DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: characteristic gender\n",
      "Template: {} is commonly associated with\n",
      "Data: 18 train, 12 test\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE, \"r\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "# Handle new format with \"samples\" key and metadata\n",
    "if isinstance(data_json, dict) and \"samples\" in data_json:\n",
    "    data = data_json[\"samples\"]\n",
    "    # Use prompt template from the data file if available\n",
    "    if \"prompt_templates\" in data_json and data_json[\"prompt_templates\"]:\n",
    "        TEMPLATE = data_json[\"prompt_templates\"][0]\n",
    "    print(f\"Dataset: {data_json.get('name', 'Unknown')}\")\n",
    "    print(f\"Template: {TEMPLATE}\")\n",
    "\n",
    "random.seed(42)  # Set seed for reproducibility\n",
    "random.shuffle(data)\n",
    "split_idx = int(len(data) * 0.6)\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "print(f\"Data: {len(train_data)} train, {len(test_data)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "Initialize the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-3-1b-pt on mps...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use environment variable for HuggingFace token\n",
    "# Set it in your terminal: export HF_TOKEN=\"your_token_here\"\n",
    "hf_token = os.getenv('HF_TOKEN', None)\n",
    "\n",
    "lre = LREModel(\n",
    "    model_name=MODEL_NAME, \n",
    "    device=\"mps\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial testing of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using layer: model.layers.12 for model: google/gemma-3-1b-pt\n"
     ]
    }
   ],
   "source": [
    " # Configuration for few-shot learning - use all training data\n",
    "K_FOLD_SIZE = len(train_data)\n",
    "# Adjust layer based on model architecture\n",
    "if \"gemma\" in MODEL_NAME.lower():\n",
    "    # Gemma models (e.g., gemma-3-1b-pt has 18 layers: 0-17)\n",
    "    BEST_LAYER = \"model.layers.12\"\n",
    "elif \"gpt2-xl\" in MODEL_NAME.lower():\n",
    "    # GPT2-XL has 48 layers: 0-47\n",
    "    BEST_LAYER = \"transformer.h.32\"\n",
    "elif \"gpt2\" in MODEL_NAME.lower():\n",
    "    # GPT2 has 12 layers: 0-11\n",
    "    BEST_LAYER = \"transformer.h.8\"\n",
    "elif \"llama\" in MODEL_NAME.lower():\n",
    "    # LLaMA models typically have 32 layers: 0-31\n",
    "    BEST_LAYER = \"model.layers.20\"\n",
    "elif \"mistral\" in MODEL_NAME.lower():\n",
    "    # Mistral models typically have 32 layers: 0-31\n",
    "    BEST_LAYER = \"model.layers.20\"\n",
    "elif \"qwen\" in MODEL_NAME.lower():\n",
    "    # Qwen models vary, but typically 32 layers: 0-31\n",
    "    BEST_LAYER = \"model.layers.20\"\n",
    "else:\n",
    "    # Default fallback\n",
    "    BEST_LAYER = \"model.layers.10\"\n",
    "    print(f\"Warning: Unknown model architecture, using default layer: {BEST_LAYER}\")\n",
    "\n",
    "print(f\"Using layer: {BEST_LAYER} for model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Few-Shot Learning with 18 samples\n",
      "Using Layer: model.layers.12\n",
      "\n",
      "Samples: [{'subject': 'multitasking', 'object': 'women'}, {'subject': 'aggressiveness', 'object': 'men'}, {'subject': 'flexibility', 'object': 'women'}, {'subject': 'bravery', 'object': 'men'}, {'subject': 'ambition', 'object': 'men'}, {'subject': 'assertiveness', 'object': 'men'}, {'subject': 'creativity', 'object': 'women'}, {'subject': 'logical thinking', 'object': 'men'}, {'subject': 'sensitivity', 'object': 'women'}, {'subject': 'nurturing', 'object': 'women'}, {'subject': 'persuasiveness', 'object': 'men'}, {'subject': 'critical thinking', 'object': 'men'}, {'subject': 'endurance', 'object': 'men'}, {'subject': 'discipline', 'object': 'men'}, {'subject': 'resilience', 'object': 'men'}, {'subject': 'adaptability', 'object': 'women'}, {'subject': 'confidence', 'object': 'men'}, {'subject': 'competitiveness', 'object': 'men'}]\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 1/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'multitasking', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "multitasking              women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      0/1 (0.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 2/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'aggressiveness', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "aggressiveness            men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 3/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'flexibility', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "flexibility               women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 4/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'bravery', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "bravery                   men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 5/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'ambition', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "ambition                  men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 6/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'assertiveness', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "assertiveness             men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 7/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'creativity', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "creativity                women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 8/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'logical thinking', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "logical thinking          men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 9/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'sensitivity', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "sensitivity               women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 10/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'nurturing', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "nurturing                 women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      0/1 (0.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 11/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'persuasiveness', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "persuasiveness            men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 12/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'critical thinking', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "critical thinking         men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 13/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'endurance', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "endurance                 men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 14/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'discipline', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "discipline                men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 15/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'resilience', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "resilience                men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 16/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "confidence is commonly associated with men.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'adaptability', 'object': 'women'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "adaptability              women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 17/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "competitiveness is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'confidence', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "confidence                men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Fold 18/18\n",
      "============================================================\n",
      "Few-shot template:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is commonly associated with men.\n",
      "assertiveness is commonly associated with men.\n",
      "creativity is commonly associated with women.\n",
      "logical thinking is commonly associated with men.\n",
      "sensitivity is commonly associated with women.\n",
      "nurturing is commonly associated with women.\n",
      "persuasiveness is commonly associated with men.\n",
      "critical thinking is commonly associated with men.\n",
      "endurance is commonly associated with men.\n",
      "discipline is commonly associated with men.\n",
      "resilience is commonly associated with men.\n",
      "adaptability is commonly associated with women.\n",
      "confidence is commonly associated with men.\n",
      "{} is commonly associated with\n",
      "\n",
      "Test sample: {'subject': 'competitiveness', 'object': 'men'}\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "competitiveness           men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      1/1 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "AVERAGING OPERATORS\n",
      "============================================================\n",
      "\n",
      "Averaged operator created from 18 folds\n",
      "Coefficient shape: (1152, 1152)\n",
      "Bias shape: (1152,)\n",
      "\n",
      "Evaluating averaged operator on all 18 samples:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "multitasking              women           women            ✓ Correct\n",
      "aggressiveness            men             men              ✓ Correct\n",
      "flexibility               women           women            ✓ Correct\n",
      "bravery                   men             men              ✓ Correct\n",
      "ambition                  men             men              ✓ Correct\n",
      "assertiveness             men             men              ✓ Correct\n",
      "creativity                women           women            ✓ Correct\n",
      "logical thinking          men             men              ✓ Correct\n",
      "sensitivity               women           women            ✓ Correct\n",
      "nurturing                 women           women            ✓ Correct\n",
      "persuasiveness            men             men              ✓ Correct\n",
      "critical thinking         men             men              ✓ Correct\n",
      "endurance                 men             men              ✓ Correct\n",
      "discipline                men             men              ✓ Correct\n",
      "resilience                men             men              ✓ Correct\n",
      "adaptability              women           women            ✓ Correct\n",
      "confidence                men             men              ✓ Correct\n",
      "competitiveness           men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      18/18 (100.00%)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 18, 'total': 18, 'faithfulness': 1.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use all training examples for few-shot learning\n",
    "k_fold_samples = train_data.copy()\n",
    "\n",
    "print(f\"K-Fold Few-Shot Learning with {K_FOLD_SIZE} samples\")\n",
    "print(f\"Using Layer: {BEST_LAYER}\")\n",
    "print(f\"\\nSamples: {k_fold_samples}\\n\")\n",
    "\n",
    "# Leave-One-Out Cross Validation\n",
    "loo = LeaveOneOut()\n",
    "operators_list = []\n",
    "bias_list = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(loo.split(k_fold_samples)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{K_FOLD_SIZE}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    fold_train = [k_fold_samples[i] for i in train_idx]\n",
    "    fold_test = [k_fold_samples[i] for i in test_idx]\n",
    "    \n",
    "    # Create few-shot template by prepending training examples\n",
    "    few_shot_examples = \"\\n\".join([\n",
    "        TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "        for sample in fold_train\n",
    "    ])\n",
    "    few_shot_template = few_shot_examples + \"\\n\" + TEMPLATE\n",
    "    \n",
    "    print(f\"Few-shot template:\\n{few_shot_template}\\n\")\n",
    "    print(f\"Test sample: {fold_test[0]}\")\n",
    "    \n",
    "    # Train operator on this fold\n",
    "    operator = lre.train_lre(fold_train, BEST_LAYER, few_shot_template)\n",
    "    \n",
    "    # Store operator weights and bias\n",
    "    operators_list.append(operator.coef_)\n",
    "    bias_list.append(operator.intercept_)\n",
    "    \n",
    "    # Evaluate on the left-out sample\n",
    "    lre.evaluate(operator, fold_test, BEST_LAYER, few_shot_template)\n",
    "\n",
    "# Average the operators\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"AVERAGING OPERATORS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "avg_coef = np.mean(operators_list, axis=0)\n",
    "avg_bias = np.mean(bias_list, axis=0)\n",
    "\n",
    "# Create averaged operator\n",
    "averaged_operator = LinearRegression()\n",
    "averaged_operator.coef_ = avg_coef\n",
    "averaged_operator.intercept_ = avg_bias\n",
    "\n",
    "print(f\"\\nAveraged operator created from {K_FOLD_SIZE} folds\")\n",
    "print(f\"Coefficient shape: {avg_coef.shape}\")\n",
    "print(f\"Bias shape: {avg_bias.shape}\")\n",
    "\n",
    "# Create few-shot template with ALL training samples for consistent evaluation\n",
    "few_shot_examples_all = \"\\n\".join([\n",
    "    TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "    for sample in k_fold_samples\n",
    "])\n",
    "few_shot_template_all = few_shot_examples_all + \"\\n\" + TEMPLATE\n",
    "\n",
    "# Evaluate averaged operator on all k_fold_samples WITH FEW-SHOT TEMPLATE\n",
    "print(f\"\\nEvaluating averaged operator on all {K_FOLD_SIZE} samples:\")\n",
    "lre.evaluate(averaged_operator, k_fold_samples, BEST_LAYER, few_shot_template_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING ON TEST SET\n",
      "============================================================\n",
      "\n",
      "Few-shot template with 18 training examples\n",
      "\n",
      "Evaluating averaged operator on 12 test samples:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 9, 'total': 12, 'faithfulness': 0.75}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate averaged operator on test set\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Create few-shot template with all training examples\n",
    "few_shot_examples_full = \"\\n\".join([\n",
    "    TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "    for sample in train_data\n",
    "])\n",
    "few_shot_template_full = few_shot_examples_full + \"\\n\" + TEMPLATE\n",
    "\n",
    "print(f\"\\nFew-shot template with {len(train_data)} training examples\")\n",
    "print(f\"\\nEvaluating averaged operator on {len(test_data)} test samples:\")\n",
    "lre.evaluate(averaged_operator, test_data, BEST_LAYER, few_shot_template_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer-by-Layer Experiment to determine the best layer\n",
    "\n",
    "We'll train an LRE operator for each layer and compare faithfulness scores.\n",
    "This helps us understand:\n",
    "- Where in the network relational knowledge emerges\n",
    "- Whether shallow or deep layers are more linear for this task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 12 layers for google/gemma-3-1b-pt: ['model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15']\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 4\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 4 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      5/12 (41.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 5\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 5 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      6/12 (50.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 6\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 6 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 7\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 7 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      7/12 (58.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 8\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 8 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 9\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 9 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 10\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 10 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 11\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 11 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      7/12 (58.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 12\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 12 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 13\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 13 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             men              ✓ Correct\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 14\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 14 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TESTING LAYER 15\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating Layer 15 on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      7/12 (58.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BEST LAYER: model.layers.6 with test faithfulness score: 0.8333\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAYER COMPARISON SUMMARY (TEST SET)\n",
      "================================================================================\n",
      "Layer 4: Test Faithfulness = 0.4167\n",
      "Layer 5: Test Faithfulness = 0.5000\n",
      "Layer 6: Test Faithfulness = 0.8333\n",
      "Layer 7: Test Faithfulness = 0.5833\n",
      "Layer 8: Test Faithfulness = 0.8333\n",
      "Layer 9: Test Faithfulness = 0.6667\n",
      "Layer 10: Test Faithfulness = 0.6667\n",
      "Layer 11: Test Faithfulness = 0.5833\n",
      "Layer 12: Test Faithfulness = 0.7500\n",
      "Layer 13: Test Faithfulness = 0.7500\n",
      "Layer 14: Test Faithfulness = 0.6667\n",
      "Layer 15: Test Faithfulness = 0.5833\n"
     ]
    }
   ],
   "source": [
    "# Layer-by-Layer Experiment - Testing different layers on test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "faithfulness_scores = {}\n",
    "\n",
    "if \"gemma\" in MODEL_NAME.lower():\n",
    "    # Gemma-3-270m has 18 layers (0-17), test layers 4-15\n",
    "    LAYERS_TO_TEST = [f\"model.layers.{i}\" for i in range(4, 16)]\n",
    "elif \"gpt2-xl\" in MODEL_NAME.lower():\n",
    "    # GPT2-XL has 48 layers, test a subset\n",
    "    LAYERS_TO_TEST = [f\"transformer.h.{i}\" for i in range(16, 40, 2)]\n",
    "elif \"gpt2\" in MODEL_NAME.lower():\n",
    "    # GPT2 has 12 layers (0-11)\n",
    "    LAYERS_TO_TEST = [f\"transformer.h.{i}\" for i in range(4, 12)]\n",
    "elif \"llama\" in MODEL_NAME.lower():\n",
    "    # LLaMA models typically have 32 layers\n",
    "    LAYERS_TO_TEST = [f\"model.layers.{i}\" for i in range(8, 28, 2)]\n",
    "elif \"mistral\" in MODEL_NAME.lower():\n",
    "    # Mistral models typically have 32 layers\n",
    "    LAYERS_TO_TEST = [f\"model.layers.{i}\" for i in range(8, 28, 2)]\n",
    "elif \"qwen\" in MODEL_NAME.lower():\n",
    "    # Qwen models typically have 32 layers\n",
    "    LAYERS_TO_TEST = [f\"model.layers.{i}\" for i in range(8, 28, 2)]\n",
    "else:\n",
    "    # Default fallback\n",
    "    LAYERS_TO_TEST = [f\"model.layers.{i}\" for i in range(4, 16)]\n",
    "\n",
    "print(f\"Testing {len(LAYERS_TO_TEST)} layers for {MODEL_NAME}: {LAYERS_TO_TEST}\")\n",
    "\n",
    "for layer_name in LAYERS_TO_TEST:\n",
    "    layer_num = layer_name.split(\".\")[-1]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TESTING LAYER {layer_num}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train with Leave-One-Out Cross Validation on train_data\n",
    "    loo_temp = LeaveOneOut()\n",
    "    layer_operators_list = []\n",
    "    layer_bias_list = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(loo_temp.split(train_data)):\n",
    "        fold_train = [train_data[i] for i in train_idx]\n",
    "        \n",
    "        # Create few-shot template for this fold\n",
    "        few_shot_examples = \"\\n\".join([\n",
    "            TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in fold_train\n",
    "        ])\n",
    "        few_shot_template = few_shot_examples + \"\\n\" + TEMPLATE\n",
    "        \n",
    "        # Train operator on this fold\n",
    "        operator_fold = lre.train_lre(fold_train, layer_name, few_shot_template)\n",
    "        \n",
    "        # Store operator weights and bias\n",
    "        layer_operators_list.append(operator_fold.coef_)\n",
    "        layer_bias_list.append(operator_fold.intercept_)\n",
    "    \n",
    "    # Average the operators for this layer\n",
    "    avg_coef_layer = np.mean(layer_operators_list, axis=0)\n",
    "    avg_bias_layer = np.mean(layer_bias_list, axis=0)\n",
    "    \n",
    "    # Create averaged operator\n",
    "    averaged_operator_layer = LinearRegression()\n",
    "    averaged_operator_layer.coef_ = avg_coef_layer\n",
    "    averaged_operator_layer.intercept_ = avg_bias_layer\n",
    "    \n",
    "    # Create few-shot template with all training examples for evaluation\n",
    "    few_shot_examples_full = \"\\n\".join([\n",
    "        TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "        for sample in train_data\n",
    "    ])\n",
    "    few_shot_template_full = few_shot_examples_full + \"\\n\" + TEMPLATE\n",
    "    \n",
    "    # Evaluate on test_data\n",
    "    print(f\"\\nEvaluating Layer {layer_num} on test set:\")\n",
    "    eval_results = lre.evaluate(averaged_operator_layer, test_data, layer_name, few_shot_template_full)\n",
    "    \n",
    "    # Store results\n",
    "    results[layer_name] = averaged_operator_layer\n",
    "    faithfulness_scores[layer_name] = eval_results.get('faithfulness', 0)\n",
    "\n",
    "# Find the best layer based on test performance\n",
    "best_layer = max(faithfulness_scores, key=faithfulness_scores.get)\n",
    "best_faithfulness = faithfulness_scores[best_layer]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST LAYER: {best_layer} with test faithfulness score: {best_faithfulness:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create faithfulness heatmap\n",
    "layer_numbers = [int(layer.split(\".\")[-1]) for layer in LAYERS_TO_TEST]\n",
    "faithfulness_values = [faithfulness_scores[layer] for layer in LAYERS_TO_TEST]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(layer_numbers, faithfulness_values, color='steelblue')\n",
    "ax.set_xlabel('Faithfulness Score (Test Set)')\n",
    "ax.set_ylabel('Layer Number')\n",
    "ax.set_title('LRE Faithfulness by Layer (Test Set Performance)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LAYER COMPARISON SUMMARY (TEST SET)\")\n",
    "print(f\"{'='*80}\")\n",
    "for layer in LAYERS_TO_TEST:\n",
    "\n",
    "    layer_num = layer.split(\".\")[-1]\n",
    "    print(f\"Layer {layer_num}: Test Faithfulness = {faithfulness_scores[layer]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Layer: model.layers.6\n",
      "Best Faithfulness Score: 0.8333\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Layer: {best_layer}\")\n",
    "print(f\"Best Faithfulness Score: {best_faithfulness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted operator from best layer: model.layers.6\n",
      "Operator coefficient shape: (1152, 1152)\n",
      "Operator intercept shape: (1152,)\n",
      "\n",
      "Best operator coefficients (first 5x5):\n",
      "[[ 4.87300049e-08 -5.73913574e-08 -7.49125419e-08 -1.77133515e-07\n",
      "   3.89995378e-08]\n",
      " [-8.73382589e-07  1.02861884e-06  1.34265122e-06  3.17475224e-06\n",
      "  -6.98986241e-07]\n",
      " [-7.35823278e-07  8.66609184e-07  1.13118062e-06  2.67471751e-06\n",
      "  -5.88894864e-07]\n",
      " [ 9.07999492e-07 -1.06939035e-06 -1.39586962e-06 -3.30058970e-06\n",
      "   7.26690246e-07]\n",
      " [ 8.52774775e-08 -1.00434974e-07 -1.31097082e-07 -3.09983989e-07\n",
      "   6.82491503e-08]]\n",
      "\n",
      "Best operator intercept (first 10):\n",
      "[-0.01281011 -0.07976672 -0.05047601  0.05980347  0.01225027 -0.04615426\n",
      " -0.02021027  0.04172536  0.06826571 -0.05290769]\n"
     ]
    }
   ],
   "source": [
    "# Extract the best operator from the layer-by-layer experiment\n",
    "best_operator = results[best_layer]\n",
    "\n",
    "print(f\"Extracted operator from best layer: {best_layer}\")\n",
    "print(f\"Operator coefficient shape: {best_operator.coef_.shape}\")\n",
    "print(f\"Operator intercept shape: {best_operator.intercept_.shape}\")\n",
    "\n",
    "# Print operator tensors\n",
    "print(f\"\\nBest operator coefficients (first 5x5):\")\n",
    "print(best_operator.coef_[:5, :5])\n",
    "\n",
    "print(f\"\\nBest operator intercept (first 10):\")\n",
    "print(best_operator.intercept_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROMPT PERTURBATION EXPERIMENT\n",
      "================================================================================\n",
      "Training with 5 prompt variations\n",
      "Using best layer: model.layers.6\n",
      "\n",
      "============================================================\n",
      "Variation 1/5: '{} is commonly associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating with same prompt variation on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Variation 2/5: '{} is typically associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating with same prompt variation on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Variation 3/5: '{} is often associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating with same prompt variation on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Variation 4/5: '{} is usually associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating with same prompt variation on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Variation 5/5: '{} is generally associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating with same prompt variation on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROMPT PERTURBATION SUMMARY\n",
      "================================================================================\n",
      "'{} is commonly associated with': Faithfulness = 0.8333\n",
      "'{} is typically associated with': Faithfulness = 0.7500\n",
      "'{} is often associated with': Faithfulness = 0.7500\n",
      "'{} is usually associated with': Faithfulness = 0.7500\n",
      "'{} is generally associated with': Faithfulness = 0.7500\n",
      "\n",
      "Best prompt variation: '{} is commonly associated with'\n",
      "Best faithfulness score: 0.8333\n",
      "\n",
      "================================================================================\n",
      "CROSS-EVALUATION: Training vs Inference Prompt Mismatch\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      8/12 (66.67%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      9/12 (75.00%)\n",
      "================================================================================\n",
      "\n",
      "Cross-Evaluation Matrix (Faithfulness Scores):\n",
      "Train Prompt                             | Test Prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "1. {} is commonly associated with      | 0.833 | 0.750 | 0.750 | 0.750 | 0.750\n",
      "2. {} is typically associated with     | 0.833 | 0.750 | 0.833 | 0.750 | 0.750\n",
      "3. {} is often associated with         | 0.750 | 0.750 | 0.750 | 0.750 | 0.667\n",
      "4. {} is usually associated with       | 0.750 | 0.667 | 0.750 | 0.750 | 0.667\n",
      "5. {} is generally associated with     | 0.833 | 0.750 | 0.833 | 0.750 | 0.750\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Prompt Perturbation Experiment\n",
    "\n",
    "# Define prompt variations\n",
    "prompt_variations = [\n",
    "    \"{} is commonly associated with\",\n",
    "    \"{} is typically associated with\",\n",
    "    \"{} is often associated with\",\n",
    "    \"{} is usually associated with\",\n",
    "    \"{} is generally associated with\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROMPT PERTURBATION EXPERIMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Training with {len(prompt_variations)} prompt variations\")\n",
    "print(f\"Using best layer: {best_layer}\")\n",
    "\n",
    "# Store operators trained with different prompts\n",
    "perturbed_operators = {}\n",
    "perturbed_scores = {}\n",
    "\n",
    "for i, prompt_template in enumerate(prompt_variations):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variation {i+1}/{len(prompt_variations)}: '{prompt_template}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train with Leave-One-Out Cross Validation using this prompt variation\n",
    "    loo_perturb = LeaveOneOut()\n",
    "    perturb_operators_list = []\n",
    "    perturb_bias_list = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(loo_perturb.split(train_data)):\n",
    "        fold_train = [train_data[i] for i in train_idx]\n",
    "        \n",
    "        # Create few-shot template with perturbed prompt\n",
    "        few_shot_examples = \"\\n\".join([\n",
    "            prompt_template.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in fold_train\n",
    "        ])\n",
    "        few_shot_template_perturb = few_shot_examples + \"\\n\" + prompt_template\n",
    "        \n",
    "        # Train operator on this fold with perturbed prompt\n",
    "        operator_fold = lre.train_lre(fold_train, best_layer, few_shot_template_perturb)\n",
    "        \n",
    "        # Store operator weights and bias\n",
    "        perturb_operators_list.append(operator_fold.coef_)\n",
    "        perturb_bias_list.append(operator_fold.intercept_)\n",
    "    \n",
    "    # Average the operators for this prompt variation\n",
    "    avg_coef_perturb = np.mean(perturb_operators_list, axis=0)\n",
    "    avg_bias_perturb = np.mean(perturb_bias_list, axis=0)\n",
    "    \n",
    "    # Create averaged operator\n",
    "    averaged_operator_perturb = LinearRegression()\n",
    "    averaged_operator_perturb.coef_ = avg_coef_perturb\n",
    "    averaged_operator_perturb.intercept_ = avg_bias_perturb\n",
    "    \n",
    "    # Create few-shot template with all training examples using same prompt variation\n",
    "    few_shot_examples_full = \"\\n\".join([\n",
    "        prompt_template.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "        for sample in train_data\n",
    "    ])\n",
    "    few_shot_template_full_perturb = few_shot_examples_full + \"\\n\" + prompt_template\n",
    "    \n",
    "    # Evaluate on test_data with the same perturbed prompt\n",
    "    print(f\"\\nEvaluating with same prompt variation on test set:\")\n",
    "    eval_results_perturb = lre.evaluate(\n",
    "        averaged_operator_perturb, \n",
    "        test_data, \n",
    "        best_layer, \n",
    "        few_shot_template_full_perturb\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    perturbed_operators[prompt_template] = averaged_operator_perturb\n",
    "    perturbed_scores[prompt_template] = eval_results_perturb.get('faithfulness', 0)\n",
    "\n",
    "# Summary of perturbation experiment\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROMPT PERTURBATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "for prompt_template in prompt_variations:\n",
    "    print(f\"'{prompt_template}': Faithfulness = {perturbed_scores[prompt_template]:.4f}\")\n",
    "\n",
    "# Find best performing prompt variation\n",
    "best_prompt = max(perturbed_scores, key=perturbed_scores.get)\n",
    "best_prompt_score = perturbed_scores[best_prompt]\n",
    "print(f\"\\nBest prompt variation: '{best_prompt}'\")\n",
    "print(f\"Best faithfulness score: {best_prompt_score:.4f}\")\n",
    "\n",
    "# Cross-evaluation: Test each operator with different prompts at inference\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CROSS-EVALUATION: Training vs Inference Prompt Mismatch\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cross_eval_results = {}\n",
    "for train_prompt in prompt_variations:\n",
    "    cross_eval_results[train_prompt] = {}\n",
    "    operator = perturbed_operators[train_prompt]\n",
    "    \n",
    "    for test_prompt in prompt_variations:\n",
    "        # Create few-shot template with test prompt\n",
    "        few_shot_examples_test = \"\\n\".join([\n",
    "            test_prompt.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in train_data\n",
    "        ])\n",
    "        few_shot_template_test = few_shot_examples_test + \"\\n\" + test_prompt\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = lre.evaluate(\n",
    "            operator, \n",
    "            test_data, \n",
    "            best_layer, \n",
    "            few_shot_template_test\n",
    "        )\n",
    "        cross_eval_results[train_prompt][test_prompt] = eval_results.get('faithfulness', 0)\n",
    "\n",
    "# Display cross-evaluation matrix\n",
    "print(\"\\nCross-Evaluation Matrix (Faithfulness Scores):\")\n",
    "print(f\"{'Train Prompt':<40} | {'Test Prompts'}\")\n",
    "print(\"-\" * 120)\n",
    "for i, train_prompt in enumerate(prompt_variations):\n",
    "    scores = [f\"{cross_eval_results[train_prompt][test_prompt]:.3f}\" \n",
    "              for test_prompt in prompt_variations]\n",
    "    print(f\"{i+1}. {train_prompt:<35} | {' | '.join(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPERATOR DIAGNOSTICS: Are the operators actually different?\n",
      "================================================================================\n",
      "\n",
      "Prompt 1 vs Prompt 2:\n",
      "  Coefficient difference: 0.000586\n",
      "  Bias difference: 0.003334\n",
      "\n",
      "Prompt 1 vs Prompt 3:\n",
      "  Coefficient difference: 0.000469\n",
      "  Bias difference: 0.002720\n",
      "\n",
      "Prompt 1 vs Prompt 4:\n",
      "  Coefficient difference: 0.000704\n",
      "  Bias difference: 0.007328\n",
      "\n",
      "Prompt 1 vs Prompt 5:\n",
      "  Coefficient difference: 0.000499\n",
      "  Bias difference: 0.005117\n",
      "\n",
      "Prompt 2 vs Prompt 3:\n",
      "  Coefficient difference: 0.000678\n",
      "  Bias difference: 0.006054\n",
      "\n",
      "Prompt 2 vs Prompt 4:\n",
      "  Coefficient difference: 0.000518\n",
      "  Bias difference: 0.003995\n",
      "\n",
      "Prompt 2 vs Prompt 5:\n",
      "  Coefficient difference: 0.000560\n",
      "  Bias difference: 0.001783\n",
      "\n",
      "Prompt 3 vs Prompt 4:\n",
      "  Coefficient difference: 0.000653\n",
      "  Bias difference: 0.010048\n",
      "\n",
      "Prompt 3 vs Prompt 5:\n",
      "  Coefficient difference: 0.000511\n",
      "  Bias difference: 0.007837\n",
      "\n",
      "Prompt 4 vs Prompt 5:\n",
      "  Coefficient difference: 0.000440\n",
      "  Bias difference: 0.002212\n"
     ]
    }
   ],
   "source": [
    "# Add this after the cross-evaluation section to verify operators are different\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OPERATOR DIAGNOSTICS: Are the operators actually different?\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Compare operator coefficients\n",
    "for i, prompt1 in enumerate(prompt_variations):\n",
    "    for j, prompt2 in enumerate(prompt_variations):\n",
    "        if i < j:  # Only compare each pair once\n",
    "            op1 = perturbed_operators[prompt1]\n",
    "            op2 = perturbed_operators[prompt2]\n",
    "            \n",
    "            # Calculate coefficient difference (Frobenius norm)\n",
    "            coef_diff = np.linalg.norm(op1.coef_ - op2.coef_)\n",
    "            bias_diff = np.abs(op1.intercept_ - op2.intercept_).mean()\n",
    "            \n",
    "            print(f\"\\nPrompt {i+1} vs Prompt {j+1}:\")\n",
    "            print(f\"  Coefficient difference: {coef_diff:.6f}\")\n",
    "            print(f\"  Bias difference: {bias_diff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPERATOR DIFFERENCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Comparing operator coefficient differences:\n",
      "\n",
      "'{} is commonly associated with' vs '{} is typically associated with':\n",
      "  Mean absolute coefficient difference: 2.54e-07\n",
      "  Mean absolute bias difference: 3.33e-03\n",
      "\n",
      "'{} is commonly associated with' vs '{} is often associated with':\n",
      "  Mean absolute coefficient difference: 1.99e-07\n",
      "  Mean absolute bias difference: 2.72e-03\n",
      "\n",
      "'{} is commonly associated with' vs '{} is usually associated with':\n",
      "  Mean absolute coefficient difference: 2.89e-07\n",
      "  Mean absolute bias difference: 7.33e-03\n",
      "\n",
      "'{} is commonly associated with' vs '{} is generally associated with':\n",
      "  Mean absolute coefficient difference: 1.95e-07\n",
      "  Mean absolute bias difference: 5.12e-03\n",
      "\n",
      "'{} is typically associated with' vs '{} is often associated with':\n",
      "  Mean absolute coefficient difference: 2.99e-07\n",
      "  Mean absolute bias difference: 6.05e-03\n",
      "\n",
      "'{} is typically associated with' vs '{} is usually associated with':\n",
      "  Mean absolute coefficient difference: 2.28e-07\n",
      "  Mean absolute bias difference: 3.99e-03\n",
      "\n",
      "'{} is typically associated with' vs '{} is generally associated with':\n",
      "  Mean absolute coefficient difference: 2.47e-07\n",
      "  Mean absolute bias difference: 1.78e-03\n",
      "\n",
      "'{} is often associated with' vs '{} is usually associated with':\n",
      "  Mean absolute coefficient difference: 2.69e-07\n",
      "  Mean absolute bias difference: 1.00e-02\n",
      "\n",
      "'{} is often associated with' vs '{} is generally associated with':\n",
      "  Mean absolute coefficient difference: 2.19e-07\n",
      "  Mean absolute bias difference: 7.84e-03\n",
      "\n",
      "'{} is usually associated with' vs '{} is generally associated with':\n",
      "  Mean absolute coefficient difference: 1.88e-07\n",
      "  Mean absolute bias difference: 2.21e-03\n",
      "\n",
      "================================================================================\n",
      "Operator magnitude check:\n",
      "\n",
      "'{} is commonly associated with':\n",
      "  Mean |coefficient|: 1.92e-06\n",
      "  Mean |bias|: 4.91e-02\n",
      "\n",
      "'{} is typically associated with':\n",
      "  Mean |coefficient|: 1.99e-06\n",
      "  Mean |bias|: 4.62e-02\n",
      "\n",
      "'{} is often associated with':\n",
      "  Mean |coefficient|: 1.90e-06\n",
      "  Mean |bias|: 5.15e-02\n",
      "\n",
      "'{} is usually associated with':\n",
      "  Mean |coefficient|: 1.95e-06\n",
      "  Mean |bias|: 4.27e-02\n",
      "\n",
      "'{} is generally associated with':\n",
      "  Mean |coefficient|: 1.93e-06\n",
      "  Mean |bias|: 4.46e-02\n"
     ]
    }
   ],
   "source": [
    "# Add this after the prompt perturbation experiment completes\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OPERATOR DIFFERENCE ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Compare operator weights to see if they're actually different\n",
    "prompt_list = list(prompt_variations)\n",
    "print(\"\\nComparing operator coefficient differences:\")\n",
    "for i in range(len(prompt_list)):\n",
    "    for j in range(i+1, len(prompt_list)):\n",
    "        op1 = perturbed_operators[prompt_list[i]]\n",
    "        op2 = perturbed_operators[prompt_list[j]]\n",
    "        \n",
    "        # Calculate the difference between operators\n",
    "        coef_diff = np.abs(op1.coef_ - op2.coef_).mean()\n",
    "        bias_diff = np.abs(op1.intercept_ - op2.intercept_).mean()\n",
    "        \n",
    "        print(f\"\\n'{prompt_list[i]}' vs '{prompt_list[j]}':\")\n",
    "        print(f\"  Mean absolute coefficient difference: {coef_diff:.2e}\")\n",
    "        print(f\"  Mean absolute bias difference: {bias_diff:.2e}\")\n",
    "\n",
    "# Check if operators are substantially different from zero\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Operator magnitude check:\")\n",
    "for prompt_template in prompt_variations:\n",
    "    op = perturbed_operators[prompt_template]\n",
    "    coef_mag = np.abs(op.coef_).mean()\n",
    "    bias_mag = np.abs(op.intercept_).mean()\n",
    "    print(f\"\\n'{prompt_template}':\")\n",
    "    print(f\"  Mean |coefficient|: {coef_mag:.2e}\")\n",
    "    print(f\"  Mean |bias|: {bias_mag:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ROBUSTNESS TEST: Prepending Text to Prompt Template\n",
      "================================================================================\n",
      "Using layer: model.layers.6\n",
      "Base template: '{} is commonly associated with'\n",
      "\n",
      "============================================================\n",
      "Testing with prepended text: ''\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Modified template structure:\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is commonly associated with men.\n",
      "ambition is comm...\n",
      "\n",
      "Evaluating with prepended text on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Testing with prepended text: 'The next few phrases are completely wrong.'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Modified template structure:\n",
      "The next few phrases are completely wrong.\n",
      "multitasking is commonly associated with women.\n",
      "aggressiveness is commonly associated with men.\n",
      "flexibility is commonly associated with women.\n",
      "bravery is com...\n",
      "\n",
      "Evaluating with prepended text on test set:\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           women            ✓ Correct\n",
      "humility                  women           women            ✓ Correct\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           women            ✓ Correct\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           women            ✓ Correct\n",
      "intuition                 women           women            ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      10/12 (83.33%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ROBUSTNESS TEST SUMMARY\n",
      "================================================================================\n",
      "(baseline): Faithfulness = 0.8333\n",
      "'The next few phrases are completely wrong.': Faithfulness = 0.8333\n",
      "\n",
      "Faithfulness variance: 0.000000\n",
      "Faithfulness std dev: 0.000000\n",
      "\n",
      "Stored 2 operators for later analysis\n"
     ]
    }
   ],
   "source": [
    "# Robustness Test: Prepending text to the prompt template\n",
    "\n",
    "prepended_texts = [\n",
    "    \"\",  # baseline - no prepending\n",
    "    \"The next few phrases are completely wrong.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ROBUSTNESS TEST: Prepending Text to Prompt Template\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Using layer: {best_layer}\")\n",
    "print(f\"Base template: '{TEMPLATE}'\")\n",
    "\n",
    "robustness_results = {}\n",
    "robustness_operators = {}  # Store operators for later analysis\n",
    "\n",
    "for prepend_text in prepended_texts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with prepended text: '{prepend_text}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train with Leave-One-Out Cross Validation using prepended text\n",
    "    loo_robust = LeaveOneOut()\n",
    "    robust_operators_list = []\n",
    "    robust_bias_list = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(loo_robust.split(train_data)):\n",
    "        fold_train = [train_data[i] for i in train_idx]\n",
    "        \n",
    "        # Create few-shot template with prepended text\n",
    "        few_shot_examples = \"\\n\".join([\n",
    "            TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in fold_train\n",
    "        ])\n",
    "        \n",
    "        if prepend_text:\n",
    "            few_shot_template_robust = prepend_text + \"\\n\" + few_shot_examples + \"\\n\" + TEMPLATE\n",
    "        else:\n",
    "            few_shot_template_robust = few_shot_examples + \"\\n\" + TEMPLATE\n",
    "        \n",
    "        # Train operator on this fold with prepended text\n",
    "        operator_fold = lre.train_lre(fold_train, best_layer, few_shot_template_robust)\n",
    "        \n",
    "        # Store operator weights and bias\n",
    "        robust_operators_list.append(operator_fold.coef_)\n",
    "        robust_bias_list.append(operator_fold.intercept_)\n",
    "    \n",
    "    # Average the operators for this prepended text\n",
    "    avg_coef_robust = np.mean(robust_operators_list, axis=0)\n",
    "    avg_bias_robust = np.mean(robust_bias_list, axis=0)\n",
    "    \n",
    "    # Create averaged operator\n",
    "    averaged_operator_robust = LinearRegression()\n",
    "    averaged_operator_robust.coef_ = avg_coef_robust\n",
    "    averaged_operator_robust.intercept_ = avg_bias_robust\n",
    "    \n",
    "    # Store the operator\n",
    "    robustness_operators[prepend_text] = averaged_operator_robust\n",
    "    \n",
    "    # Create few-shot template with all training examples and prepended text for evaluation\n",
    "    few_shot_examples_full = \"\\n\".join([\n",
    "        TEMPLATE.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "        for sample in train_data\n",
    "    ])\n",
    "    \n",
    "    if prepend_text:\n",
    "        few_shot_template_full_robust = prepend_text + \"\\n\" + few_shot_examples_full + \"\\n\" + TEMPLATE\n",
    "    else:\n",
    "        few_shot_template_full_robust = few_shot_examples_full + \"\\n\" + TEMPLATE\n",
    "    \n",
    "    print(f\"Modified template structure:\")\n",
    "    print(f\"{few_shot_template_full_robust[:200]}...\")\n",
    "    \n",
    "    # Evaluate on test_data with the same prepended text\n",
    "    print(f\"\\nEvaluating with prepended text on test set:\")\n",
    "    eval_results_robust = lre.evaluate(\n",
    "        averaged_operator_robust,\n",
    "        test_data,\n",
    "        best_layer,\n",
    "        few_shot_template_full_robust\n",
    "    )\n",
    "    \n",
    "    robustness_results[prepend_text] = eval_results_robust.get('faithfulness', 0)\n",
    "\n",
    "# Summary of robustness test\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ROBUSTNESS TEST SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "for prepend_text in prepended_texts:\n",
    "    label = f\"'{prepend_text}'\" if prepend_text else \"(baseline)\"\n",
    "    print(f\"{label}: Faithfulness = {robustness_results[prepend_text]:.4f}\")\n",
    "\n",
    "# Calculate faithfulness variance\n",
    "faithfulness_values_robust = list(robustness_results.values())\n",
    "faithfulness_variance = np.var(faithfulness_values_robust)\n",
    "faithfulness_std = np.std(faithfulness_values_robust)\n",
    "\n",
    "print(f\"\\nFaithfulness variance: {faithfulness_variance:.6f}\")\n",
    "print(f\"Faithfulness std dev: {faithfulness_std:.6f}\")\n",
    "\n",
    "print(f\"\\nStored {len(robustness_operators)} operators for later analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Eigenvalues Comparison:\n",
      "Rank   Baseline        Prepended       Diff           \n",
      "-------------------------------------------------------\n",
      "1      9.936694e-05    1.077668e-04    8.399904e-06   \n",
      "2      3.957920e-11    2.144491e-11    1.813429e-11   \n",
      "3      1.780296e-11    1.856492e-11    7.619582e-13   \n",
      "4      1.610888e-11    1.856492e-11    2.456037e-12   \n",
      "5      1.610888e-11    1.685960e-11    7.507172e-13   \n"
     ]
    }
   ],
   "source": [
    "# Plot comparing top 5 eigenvalues\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(5)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_top5, width, label='Baseline (no prepending)', alpha=0.8, color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, prepended_top5, width, label='Prepended operator', alpha=0.8, color='coral')\n",
    "\n",
    "ax.set_xlabel('Eigenvalue Rank', fontsize=12)\n",
    "ax.set_ylabel('Magnitude', fontsize=12)\n",
    "ax.set_title('Top 5 Eigenvalues: Baseline vs Prepended Operator', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{i+1}' for i in range(5)])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2e}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison statistics\n",
    "print(f\"\\nTop 5 Eigenvalues Comparison:\")\n",
    "print(f\"{'Rank':<6} {'Baseline':<15} {'Prepended':<15} {'Diff':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(5):\n",
    "    diff = abs(baseline_top5[i] - prepended_top5[i])\n",
    "    print(f\"{i+1:<6} {baseline_top5[i]:<15.6e} {prepended_top5[i]:<15.6e} {diff:<15.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROMPT PERTURBATION EXPERIMENT\n",
      "================================================================================\n",
      "Training with 5 prompt variations\n",
      "Using best layer: model.layers.6\n",
      "\n",
      "============================================================\n",
      "Variation 1/5: '{} is commonly associated with'\n",
      "============================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "Extracting training representations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m few_shot_template_perturb = few_shot_examples + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + prompt_template\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Train operator on this fold with perturbed prompt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m operator_fold = \u001b[43mlre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_lre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfew_shot_template_perturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Store operator weights and bias\u001b[39;00m\n\u001b[32m     48\u001b[39m perturb_operators_list.append(operator_fold.coef_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/lre/lre.py:88\u001b[39m, in \u001b[36mLREModel.train_lre\u001b[39m\u001b[34m(self, training_data, layer_name, template)\u001b[39m\n\u001b[32m     85\u001b[39m prompt = template.format(subj)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# 1. Get Input State (s) at the specific layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m h_s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_hidden_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m X.append(h_s)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 2. Get the \"Ideal\" direction. \u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# In the original paper, they map s -> z (final layer output).\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# We approximate this by looking at the embedding of the target object.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Handle models that add special tokens (like Gemma's <bos>)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/lre/lre.py:62\u001b[39m, in \u001b[36mLREModel.get_hidden_state\u001b[39m\u001b[34m(self, text, layer_name, subject)\u001b[39m\n\u001b[32m     59\u001b[39m     subj_end_idx = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TraceDict(\u001b[38;5;28mself\u001b[39m.model, [layer_name]) \u001b[38;5;28;01mas\u001b[39;00m ret:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Extract vector: [Batch, Seq, Hidden] -> [Hidden]\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Handle both tuple output (e.g., GPT-2) and tensor output (e.g., Qwen)\u001b[39;00m\n\u001b[32m     66\u001b[39m output = ret[layer_name].output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:658\u001b[39m, in \u001b[36mGemma3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m output_hidden_states = (\n\u001b[32m    655\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    656\u001b[39m )\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:570\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    568\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:382\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    380\u001b[39m     position_embeddings = position_embeddings_global\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    394\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:327\u001b[39m, in \u001b[36mGemma3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    325\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    340\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lre-experiment/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Prompt Perturbation Experiment\n",
    "\n",
    "# Define prompt variations\n",
    "prompt_variations = [\n",
    "    \"{} is commonly associated with\",\n",
    "    \"{} is typically associated with\",\n",
    "    \"{} is often associated with\",\n",
    "    \"{} is usually associated with\",\n",
    "    \"{} is generally associated with\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROMPT PERTURBATION EXPERIMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Training with {len(prompt_variations)} prompt variations\")\n",
    "print(f\"Using best layer: {best_layer}\")\n",
    "\n",
    "# Store operators trained with different prompts\n",
    "perturbed_operators = {}\n",
    "perturbed_scores = {}\n",
    "\n",
    "for i, prompt_template in enumerate(prompt_variations):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variation {i+1}/{len(prompt_variations)}: '{prompt_template}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train with Leave-One-Out Cross Validation using this prompt variation\n",
    "    loo_perturb = LeaveOneOut()\n",
    "    perturb_operators_list = []\n",
    "    perturb_bias_list = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(loo_perturb.split(train_data)):\n",
    "        fold_train = [train_data[i] for i in train_idx]\n",
    "        \n",
    "        # Create few-shot template with perturbed prompt\n",
    "        few_shot_examples = \"\\n\".join([\n",
    "            prompt_template.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in fold_train\n",
    "        ])\n",
    "        few_shot_template_perturb = few_shot_examples + \"\\n\" + prompt_template\n",
    "        \n",
    "        # Train operator on this fold with perturbed prompt\n",
    "        operator_fold = lre.train_lre(fold_train, best_layer, few_shot_template_perturb)\n",
    "        \n",
    "        # Store operator weights and bias\n",
    "        perturb_operators_list.append(operator_fold.coef_)\n",
    "        perturb_bias_list.append(operator_fold.intercept_)\n",
    "    \n",
    "    # Average the operators for this prompt variation\n",
    "    avg_coef_perturb = np.mean(perturb_operators_list, axis=0)\n",
    "    avg_bias_perturb = np.mean(perturb_bias_list, axis=0)\n",
    "    \n",
    "    # Create averaged operator\n",
    "    averaged_operator_perturb = LinearRegression()\n",
    "    averaged_operator_perturb.coef_ = avg_coef_perturb\n",
    "    averaged_operator_perturb.intercept_ = avg_bias_perturb\n",
    "    \n",
    "    # Create few-shot template with all training examples using same prompt variation\n",
    "    few_shot_examples_full = \"\\n\".join([\n",
    "        prompt_template.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "        for sample in train_data\n",
    "    ])\n",
    "    few_shot_template_full_perturb = few_shot_examples_full + \"\\n\" + prompt_template\n",
    "    \n",
    "    # Evaluate on test_data with the same perturbed prompt\n",
    "    print(f\"\\nEvaluating with same prompt variation on test set:\")\n",
    "    eval_results_perturb = lre.evaluate(\n",
    "        averaged_operator_perturb, \n",
    "        test_data, \n",
    "        best_layer, \n",
    "        few_shot_template_full_perturb\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    perturbed_operators[prompt_template] = averaged_operator_perturb\n",
    "    perturbed_scores[prompt_template] = eval_results_perturb.get('faithfulness', 0)\n",
    "\n",
    "# Summary of perturbation experiment\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROMPT PERTURBATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "for prompt_template in prompt_variations:\n",
    "    print(f\"'{prompt_template}': Faithfulness = {perturbed_scores[prompt_template]:.4f}\")\n",
    "\n",
    "# Find best performing prompt variation\n",
    "best_prompt = max(perturbed_scores, key=perturbed_scores.get)\n",
    "best_prompt_score = perturbed_scores[best_prompt]\n",
    "print(f\"\\nBest prompt variation: '{best_prompt}'\")\n",
    "print(f\"Best faithfulness score: {best_prompt_score:.4f}\")\n",
    "\n",
    "# Cross-evaluation: Test each operator with different prompts at inference\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CROSS-EVALUATION: Training vs Inference Prompt Mismatch\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cross_eval_results = {}\n",
    "for train_prompt in prompt_variations:\n",
    "    cross_eval_results[train_prompt] = {}\n",
    "    operator = perturbed_operators[train_prompt]\n",
    "    \n",
    "    for test_prompt in prompt_variations:\n",
    "        # Create few-shot template with test prompt\n",
    "        few_shot_examples_test = \"\\n\".join([\n",
    "            test_prompt.format(sample['subject']) + f\" {sample['object']}.\"\n",
    "            for sample in train_data\n",
    "        ])\n",
    "        few_shot_template_test = few_shot_examples_test + \"\\n\" + test_prompt\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = lre.evaluate(\n",
    "            operator, \n",
    "            test_data, \n",
    "            best_layer, \n",
    "            few_shot_template_test\n",
    "        )\n",
    "        cross_eval_results[train_prompt][test_prompt] = eval_results.get('faithfulness', 0)\n",
    "\n",
    "# Display cross-evaluation matrix\n",
    "print(\"\\nCross-Evaluation Matrix (Faithfulness Scores):\")\n",
    "print(f\"{'Train Prompt':<40} | {'Test Prompts'}\")\n",
    "print(\"-\" * 120)\n",
    "for i, train_prompt in enumerate(prompt_variations):\n",
    "    scores = [f\"{cross_eval_results[train_prompt][test_prompt]:.3f}\" \n",
    "              for test_prompt in prompt_variations]\n",
    "    print(f\"{i+1}. {train_prompt:<35} | {' | '.join(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Experiment: Different Prompt Template\n",
    "\n",
    "Let's also test whether a different prompt format affects results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING ALTERNATIVE PROMPT TEMPLATE\n",
      "Template: 'Most {} majors are'\n",
      "================================================================================\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      5/12 (41.67%)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 5, 'total': 12, 'faithfulness': 0.4166666666666667}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try an alternative template\n",
    "ALT_TEMPLATE = \"Most {} majors are\"\n",
    "BEST_LAYER = \"model.layers.15\"  # Based on results above, adjust if needed\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING ALTERNATIVE PROMPT TEMPLATE\")\n",
    "print(f\"Template: '{ALT_TEMPLATE}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "operator_alt = lre.train_lre(train_data, BEST_LAYER, ALT_TEMPLATE)\n",
    "lre.evaluate(operator_alt, test_data, BEST_LAYER, ALT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STANDARD LINEAR REGRESSION (No Leave-One-Out)\n",
      "============================================================\n",
      "Training on 18 samples\n",
      "Using Layer: model.layers.15\n",
      "Extracting training representations...\n",
      "Solving Linear Regression...\n",
      "\n",
      "Evaluating on training data (18 samples):\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "multitasking              women           women            ✓ Correct\n",
      "aggressiveness            men             men              ✓ Correct\n",
      "flexibility               women           women            ✓ Correct\n",
      "bravery                   men             men              ✓ Correct\n",
      "ambition                  men             men              ✓ Correct\n",
      "assertiveness             men             men              ✓ Correct\n",
      "creativity                women           women            ✓ Correct\n",
      "logical thinking          men             men              ✓ Correct\n",
      "sensitivity               women           women            ✓ Correct\n",
      "nurturing                 women           women            ✓ Correct\n",
      "persuasiveness            men             men              ✓ Correct\n",
      "critical thinking         men             men              ✓ Correct\n",
      "endurance                 men             men              ✓ Correct\n",
      "discipline                men             men              ✓ Correct\n",
      "resilience                men             men              ✓ Correct\n",
      "adaptability              women           women            ✓ Correct\n",
      "confidence                men             men              ✓ Correct\n",
      "competitiveness           men             men              ✓ Correct\n",
      "================================================================================\n",
      "Faithfulness Score:                      18/18 (100.00%)\n",
      "================================================================================\n",
      "\n",
      "Evaluating on test data (12 samples):\n",
      "\n",
      "================================================================================\n",
      "                               EVALUATION RESULTS                               \n",
      "================================================================================\n",
      "Subject                   Expected        LRE Prediction      Status\n",
      "--------------------------------------------------------------------------------\n",
      "risk-taking               men             women              ✗ Wrong\n",
      "leadership                men             men              ✓ Correct\n",
      "meticulousness            women           men                ✗ Wrong\n",
      "decisiveness              men             men              ✓ Correct\n",
      "patience                  women           men                ✗ Wrong\n",
      "humility                  women           men                ✗ Wrong\n",
      "adventurousness           men             men              ✓ Correct\n",
      "independence              men             men              ✓ Correct\n",
      "generosity                women           men                ✗ Wrong\n",
      "empathy                   women           women            ✓ Correct\n",
      "compassion                women           men                ✗ Wrong\n",
      "intuition                 women           men                ✗ Wrong\n",
      "================================================================================\n",
      "Faithfulness Score:                      5/12 (41.67%)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correct': 5, 'total': 12, 'faithfulness': 0.4166666666666667}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Linear Regression without Leave-One-Out\n",
    "print(f\"{'='*60}\")\n",
    "print(\"STANDARD LINEAR REGRESSION (No Leave-One-Out)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training on {len(train_data)} samples\")\n",
    "print(f\"Using Layer: {BEST_LAYER}\")\n",
    "\n",
    "# Train operator on all training data\n",
    "operator_standard = lre.train_lre(train_data, BEST_LAYER, TEMPLATE)\n",
    "\n",
    "# Evaluate on training data\n",
    "print(f\"\\nEvaluating on training data ({len(train_data)} samples):\")\n",
    "lre.evaluate(operator_standard, train_data, BEST_LAYER, TEMPLATE)\n",
    "\n",
    "# Evaluate on test data\n",
    "print(f\"\\nEvaluating on test data ({len(test_data)} samples):\")\n",
    "lre.evaluate(operator_standard, test_data, BEST_LAYER, TEMPLATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lre-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
